\documentclass{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage{tcolorbox}
\usepackage{verbatim}
\usepackage{qtree}

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\E}{\mathbb{E}}

\setlength{\parindent}{0pt}

\colorlet{LightGray}{White!90!Periwinkle}
\colorlet{LightOrange}{Orange!15}
\colorlet{LightGreen}{Green!15}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}

\declaretheoremstyle[name=Theorem,]{thmsty}
\declaretheorem[style=thmsty,numberwithin=section]{theorem}
\tcolorboxenvironment{theorem}{colback=LightGray}

\declaretheoremstyle[name=Example,]{prosty}
\declaretheorem[style=prosty,numberlike=theorem]{example}
\tcolorboxenvironment{example}{colback=LightOrange}

\declaretheoremstyle[name=Definition,]{prcpsty}
\declaretheorem[style=prcpsty,numberlike=theorem]{definition}
\tcolorboxenvironment{definition}{colback=LightGreen}

\setstretch{1.2}
\geometry{
  textheight=9in,
  textwidth=5.5in,
  top=1in,
  headheight=12pt,
  headsep=25pt,
  footskip=30pt
}

% ------------------------------------------------------------------------------

\begin{document}

% ------------------------------------------------------------------------------
% Cover Page and ToC
\title{ \normalsize \textsc{}
  \\ [2.0cm]
  \HRule{1.5pt} \\
  \LARGE \textbf{\uppercase{CAS MA581 Probability}
  \HRule{2.0pt} \\ [0.6cm] \LARGE{Spring 2024} \vspace*{10\baselineskip}}
}
\date{}
\author{\textbf{Frank Yang} \\
  Professor Dhama \\
  CGS 505 \\
MWF 10:10 -- 11:00}

\maketitle
\newpage

\tableofcontents
\newpage
% ------------------------------------------------------------------------------
\begin{comment}

\section{Examples}

\begin{theorem}
    This is a theorem.
\end{theorem}

\begin{proposition}
    This is a proposition.
\end{proposition}

\begin{principle}
    This is a principle.
\end{principle}

% Maybe I need to add one more part: Examples.
% Set style and colour later.

\subsection{Pictures}

\begin{figure}[htbp]
    \center
    \includegraphics[scale=0.06]{img/photo.jpg}
    \caption{Sydney, NSW}
\end{figure}
\end{comment}
% ------------------------------------------------------------------------------
\section{Probability Basics}
\setcounter{subsection}{1}
\subsection{Set Theory}
\begin{definition}
  A \textbf{set} is a collection of elements (usually numbers).
\end{definition}
If $x$ is an element of set $A$, then \[x \in A\] otherwise, \[x \notin A.\]\\
An empty set is denoted by $\phi$.
\begin{definition}
  If every element of set $A$ is an element of set $B$, then $A$ is said to be a \textbf{subset} of $B$: $A \subset B$
\end{definition}
Two sets $A$ and $B$ are said to be equal when they have the same elements: \[A=B\implies A\subset B \text{ and } B\subset A\]
Additional notation:
\begin{definition}\text{}\\
  $\R$ is the set of real numbers\\
  $\Q$ is the set of rationals\\
  $\Z$ is the set of integers\\
  $\N$ is the set of positive integers\\
  $\therefore\N\subset\Z\subset\Q\subset\R$
\end{definition}
Interval notation:
\\Given $a,b\in\R$:\[[a,b]=\{x\in\R:a\leq x\leq b\}\]\[[a,b)=\{x\in\R:a\leq x< b\}\]
Set operations (union, intersection, complement):\\
Let $A,B,\text{ and } E$ be subsets of $U$.
\begin{itemize}
  \item The complement of $E$ is $E^c=\{x\mid x\notin E\}$
  \item Intersection: $A\cap B=\{x\mid x\in A \text{ and } x\in B\}$
  \item Union: $A\cup B=\{x\mid x\in A \text{ or } x\in B\}$
\end{itemize}
Given $A\subset B$, the occurrence of $A$ implies the occurrence of $B$. Given $A^c$, $A$ doesn't occur. Given $A\cap B$, $A$ and $B$ occur. Given $A \cup B$, $A$ or $B$ occurs.
\newpage
Venn diagrams can be used to explain these set operations:
\begin{figure}[htbp]
  \center
  \includegraphics[scale=0.5]{1.png}
\end{figure}
\begin{example}
  Given $U=\R=(-\infty,+\infty), E=[0,3], A=(-\infty, 6), B=(5, +\infty), \text{ find } E^c, A\cap B, \text{ and } A\cup B.$\\
  $E^c=(-\infty, 0)\cup(3, +\infty)$.\\
  $A\cap B=(5, 6)$.\\
  $A\cup B=(-\infty, +\infty)$.

\end{example}
\begin{theorem}
  De-Morgan's Law:
  Let $A$ and $B$ be subsets of $U$. Then,
  \begin{enumerate}
    \item $(A\cup B)^c=A^c\cap B^c$
    \item $(A\cap B)^c=A^c \cup B^c$
  \end{enumerate}
\end{theorem}
A rigorous proof of Theorem 1.5 proceeds as follows:
\[(A\cup B)^c \subset A^c\cap B^c?\]
\[\text{Let } x\in (A\cup B)^c\implies x\notin A\cup B\implies x\notin A \text{ and } x\notin B\implies\]\[x\in A^c \text{ and } x\in B^c\implies x\in A^c \cap B^c \therefore (A\cup B)^c\subset A^c\cap B^c\]
\[A^c\cap B^c \subset (A\cup B)^c?\]
\[\text{Let }x \in A^c \cap B^c\implies x\in A^c \text{ and } x \in B^c \implies\] \[x\notin A \text{ and } x\notin B\implies x\notin A\cup B\implies x\in (A\cup B)^c\therefore A^c\cap B^c\subset(A\cup B)^c\]
\[\therefore (A\cup B)^c = A^c\cap B^c\]
Some other properties:
Let $A, B, C$ be subsets of $U$. Then,
\begin{itemize}
  \item $A\cap(B\cup C) = (A\cap B)\cup (A\cap C)$
  \item $A\cup (B\cap C) = (A\cup B)\cap (A\cup C)$
  \item $A\cap B = B\cap A$
  \item $(\bigcup_{n=1}^N A_n)^c = \bigcap_{n=1}^N A_n^c$ where $\bigcap_{n=1}^N A_n = \{x\mid x\in A_n \text{ for all } n=1, 2, \dots, N\}$ and $\bigcup_{n=1}^N A_n = \{x\mid x\in A_n \text{ for all } n=1, 2, \dots, N\}$ [similarly, $\bigcap_{n=1}^\infty A_n = \{x\mid x\in A_n \text{ for all } n \in \N\}]$
\end{itemize}
\begin{example}
  $U=\R$, for each $n\in \N=\{1, 2, \dots\}$. Define $A_n=[\frac{1}{n}, 2+\frac{1}{n}$).
  \begin{enumerate}
    \item First, try $\bigcap_{n=1}^3 A_n$ and $\bigcup_{n=1}^3 A_n$.\\
      $\bigcap_{n=1}^3 A_n = A_1\cap A_2\cap A_3 = [1, 3)\cap [\frac{1}{2},\frac{5}{2})\cap [\frac{1}{3}, \frac{7}{3})=[1, \frac{7}{3})$\\
      $\bigcup_{n=1}^3 A_n = [\frac{1}{3},3)$
    \item Next, try $\bigcap_{n=1}^\infty A_n$ and $\bigcup_{n=1}^\infty A_n$.\\
      $\bigcap_{n=1}^\infty A_n = [1, 2]$ because, if we take $\frac{1}{n}$ to infinity, it converges to 0, thus $2+\frac{1}{n} = 2$.\\
      (If $x\in \bigcap_{n=1}^\infty A_n \iff\text{(if and only if) } x\in[\frac{1}{n}, 2+\frac{1}{n})$ for all $n\in \N\iff x\geq 1$ and $x\leq2\iff x\in [1,2].)$\\
      $\bigcup_{n=1}^\infty A_n = (0,3)$ because the minimum approaches, but never reaches 0, while the maximum is 3(exclusive) at $A_1$.\\
      We note, for each $n\in \N, A_n\subset (0,3) \therefore \bigcup_{n=1}^\infty A_n \subset (0,3)$.\\
      Now, to show $(0,3)\subset \bigcup_{n=1}^\infty A_n$:\\
      Let $m$ be a positive integer such that $x\geq \frac{1}{m}$. If $m=1$, $x\in[1, 3)=A_1.$ If $m\geq2, $then $\frac{1}{m}\leq x< \frac{1}{m-1}$.\\
      $\therefore \frac{1}{m}\leq x< \frac{1}{m-1}\leq1<2+\frac{1}{m}$, hence, $x\in A_m =[\frac{1}{m}, 2+\frac{1}{m})\implies x\in A_m\subset \bigcup_{n=1}^\infty A_n.$

  \end{enumerate}
\end{example}
\section{Mathematical Probability}
\subsection{Sample Space and Events}
\begin{definition}
  A \textbf{random experiment} is an experiment whose outcome can't be predicted.
\end{definition}
\begin{definition}
  The \textbf{sample space} is the set of all possible outcomes of the random experiment(denoted by $\Omega$).
\end{definition}
\begin{definition}
  An \textbf{event} is a subset of the sample space. The set of all events is denoted by $\mathcal{F}$.
\end{definition}
\begin{example}
  Tossing a coin:
  \[\Omega=\{H, T\}; \mathcal{F}=\{\phi,\{H\}, \{T\}, \Omega\}\]
  If the coin were tossed twice,
  \[\Omega=\{HH, TT, TH, HT\}; |\mathcal{F}|=2^4=16\]
\end{example}\newpage
Consider a random experiment of tossing a coin three times. Write the event that the total number of heads is two ($E=\{HHT, HTH, THH\}$).\\
Assumptions: Let $\Omega$ be the sample space. We assume that the collection of elements $\mathcal{F}$ satisfies:
\begin{enumerate}
  \item $\phi\in\mathcal{F}, \Omega\in\mathcal{F}$
  \item If $A_1, A_2, \dots$ are in $\mathcal{F}$, then $\bigcup_{n=1}^\infty A_n\in\mathcal{F}$
  \item If $A\in\mathcal{F}, A^c\in\mathcal{F}$
\end{enumerate}
\subsection{Axioms of Probability}
Let $\Omega$ be the sample space, and $\mathcal{F}$ be the collection of events. A probability(or probability measure) $\PP$ is the function from $\mathcal{F}$ to $[0,1]$. That is, $\PP:\mathcal{F}\longrightarrow[0,1]$ such that the following Kolmogorov axioms are satisfied:
\begin{enumerate}
  \item  $\PP(A)\geq0$
  \item If $A_1, \dots$ is a collection of mutually disjoint events, then $\PP(\bigcup_{n=1}^\infty A_n)=\sum_{n=1}^\infty \PP(A_n)$
  \item $\PP(\Omega)=1$
\end{enumerate}

\begin{example}
  Consider the random experiment of rolling a dice.
  \[\Omega=\{1, 2, 3, 4, 5, 6\}\]
  Event A is when we get a number divisible by 2, and Event B is when we get a number divisible by 3.
  \[A=\{2, 4, 6\}, B=\{3, 6\}\]
  \[A\cup B=\{2, 3, 4, 6\}\]
  $\PP(A)=\frac{3}{6};\PP(B)=\frac{2}{6}\\ \PP(A\cup B)=\frac{4}{6} < \PP(A)+\PP(B)=\frac{5}{6}$
\end{example}
\begin{example}
  Now consider the same experiment from Example 1.11, but where Event A is getting an odd number, and Event B is getting an even number.
  \[A=\{1, 3, 5\}, B=\{2, 4, 6\}\]
  $\PP(A)=\frac{3}{6}=\PP(B)$\\
  $\PP(A\cup B)=1=\PP(A)+\PP(B)$
\end{example}
The triple $(\Omega, \mathcal{F}, \PP$) is a called a probability space.\\
Properties of Probability:
\begin{enumerate}
  \item For any event $E$, $\PP(E^c)=1-\PP(E)$
  \item $\PP(\phi)=0$
  \item If $A\subset B$, then $\PP(A)\leq\PP(B)$
  \item (The Inclusion-Exclusion Property) Let $A$ and $B$ be two events. Then \\$\PP(A\cup B)= \PP(A)+\PP(B)-\PP(A\cap B)$
\end{enumerate}
\begin{example}
  For a community, 60\% of people play cricket, 50\% play football, and 80\% either play cricket or football. What is the probability that a randomly chosen person plays both?
  \\Let $A$ be the event that a randomly chosen person plays cricket, and $B$ be the event that a randomly chosen person plays football. We are given that $\PP(A)=0.6, \PP(B)=0.5, \PP(A\cup B)=0.8$.\\
  $\implies 0.8=0.6+0.5-\PP(A\cap B)\therefore \PP(A\cap B)=0.3$
\end{example}
\subsection{Specifying Probabilities}
\begin{definition}
  Proposition: (Probabilities for countable sample space) Let $\Omega$ be a countable sample space. Then, for each event $E$ we have
  \begin{enumerate}
    \item $\PP(E)=\sum_{\omega\in A} \PP(\{\omega\})$
    \item $\sum_{\omega\in A} \PP(\{\omega\})=1$
  \end{enumerate}
  Also, the number of subsets in $\Omega=\{1, \dots, N\}$ is $2^N$.
\end{definition}
\subsubsection{Classical Probability}
Let $\Omega$ be a finite sample space with, say $N$, equally likely outcomes of a random experiment.
Here, $\Omega = \{\omega_1, \omega_2, \dots, \omega_N\}$ and $\phi$ denotes the probability of each outcome. By the probability axioms,
\[1=\sum_{i=1}^N \PP(\{\omega_i\})=\sum_{i=1}^N \phi = N\phi\implies \phi = \frac{1}{N}\]
Result: Let $\Omega$ be a finite sample space where each outcome is equally likely. Then, for each event $A$,
\[\PP(A) = \frac{N(A)}{N(\Omega)},\]
where $N(A)$: number of ways that event $A$ can occur, $N(\Omega)$: total $n$ of possible outcomes.
\begin{example}
  Consider the random experiment of rolling two balanced dices. Calculate the probability of the event that
  \begin{enumerate}
    \item The sum of the dice is 5.
    \item Both dices come up the same number.
  \end{enumerate}
  Solution: The sample space for this experiment is
  \[\Omega = \{(i,j):1\leq i\leq6, 1\leq j\leq6\}=\{(1,1),(1,2),\dots\}\]
  \[\text{so, }N(\Omega)=6^2=36\]
  For each event $E$, we have
  \[\PP(E)=\frac{N(E)}{N(\Omega)}=\frac{N(E)}{36}\]
  \begin{enumerate}
    \item Let $A$ be the event that the sum of the dice is 5. Therefore, $A=\{(1,4),(2,3),(3,2),(4,1)\}.$ Hence,
      \[\PP(A)=\frac{N(A)}{36}=\frac{4}{36}=\frac{1}{9}\]
    \item Let $B$ be the event that the dices come up the same number. Therefore, $B=\{(1,1),(2,2),\dots,(6,6)\}$.
      \[\PP(B)=\frac{N(B)}{N(\Omega)}=\frac{6}{36}=\frac{1}{6}\]
  \end{enumerate}
\end{example}
\subsubsection{Geometric Probability}
Consider a sample space that is a subset of $\R^n$. Let $A,B$ be two subsets,
\[A\times B=\{(a,b):a\in A, b\in B\}\]
\[\R^n=\R\times\R\times\dots\times\R=\{(x_i, \dots, x_n)\mid x_i\in \R, 1\leq i\leq n\}\]
Because of the geometric nature of sample space, the equal likelihood model in this context is called geometric probability.\\
Let $\Omega$ be a subset of $\R^2 = \R\times\R$, and $E$ be a subset of $\Omega$. This could be a random experiment of collecting a point at random from $\Omega$.\\Sample space is $\Omega$. $E$ is the event that the point selected is an element of subset $E$. Since the point is selected at random,
\[\PP(E)\propto area(E)\implies\exists\text{ a constant $k$ such}\]
\[\PP(E)=k\cdot area(E), \text{for all events }E.\]
Now, we determine $k$: let $E=\Omega$, by certainty axiom, $1=\PP(\Omega)=k\cdot area(\Omega)\implies k=\frac{1}{area(\Omega)}$. Hence, $\PP(E)=\frac{area(E)}{area(\Omega)}$.\\
Note: In higher dimensions, the area can be replaced by volume.
\begin{example}
  Suppose that a number is selected at random from $(0,1)$. Determine the probability that the number obtained is
  \begin{enumerate}
    \item 0.25 or greater
    \item between 0.1 and 0.4, inclusive
    \item either less than 0.1 or greater than 0.4
  \end{enumerate}
  Solution: $\Omega = (0,1)$. Recall for event $E$, $\PP(E)=\frac{length(E)}{length(\Omega)}, \PP(E) = length(E)\therefore length(\Omega)=1$.
  \begin{enumerate}
    \item Event $E$ is the subset $[0.25,1)$. $\PP(E)=length([0.25,1))=0.75$
    \item Let $E$ denote the event that the number selected is between 0.1 and 0.4. $E=[0.1,0.4]\subset=(0,1), \PP(E)=length(E)=0.3$
    \item Here, $E = (0,0.1)\cup [0.4,1)\subset(0,1), \PP(E)=\PP((0,0.1))+((0.4,1))=0.7$
  \end{enumerate}
\end{example}
\subsection{Law of Partitions}
\begin{definition}
  Events $A_1, A_2, \dots$ form a partition of $\Omega$ if
  \begin{enumerate}
    \item mutually exclusive ($A_n\cap A_m=\phi$ for $m\neq n$)
    \item exhaustive: $\bigcap_{n\geq1} A_n=\Omega$
  \end{enumerate}
\end{definition}
Any event E and its complement form a partition.
\begin{figure}[htbp]
  \center
  \includegraphics[scale=1]{2.png}
  \caption{$A=(B_1\cap A)\cup(B_2\cap A)\cup(B_3 \cap A)$}
\end{figure}
\\Result: From Figure 1, suppose that $B_1, \dots$ form a partition of $S$. Then,
\begin{enumerate}
  \item $\PP(A)=\sum_{n=1}^\infty \PP(B_n\cap A)$, for any event $A$
  \item If $E$ is an event, then $\PP(A)=\PP(E\cap A)+\PP(E^c\cap A)\forall A$
\end{enumerate}
\begin{example}
  According to a set of data, 8.1\% of institutions of higher education are public schools in the Northeast, 11\% are in the Midwest, 16.3\% are in the South, and 9.6\% are in the West. If a US institution of higher education is selected at random, determine the probability that it is public.
  \\Solution: Let $A_1$ be the event that the school selected is in the Northeast, $A_2$ in the Midwest, $A_3$ in the South, and $A_4$ in the West. Let $B$ be the event that the school selected is public.\\
  $\PP(A_1\cap B)=0.081, \PP(A_2\cap B)=0.11, \PP(A_3\cap B)=0.163, \PP(A_4\cap B)=0.096.$\\
  By the law of partitions, $\PP(B)=\PP(A_1\cap B) + \PP(A_2\cap B)+\PP(A_3\cap B)+\PP(A_4\cap B) = 0.45.$ 45\% of institutions of higher education are public.
\end{example}
\section{Combinatorial Probability}
\subsection{Basic counting rules}
\begin{itemize}
  \item $r$ actions (choices/experiments) in definitive order
  \item $m_1$ possibilities for the first action, $m_2$ for the second, $\dots$, $m_r$ for the $r^{th}$ action
\end{itemize}
Then, there are $m_1, m_2, \dots, m_r$ possibilities altogether for the $r^{th}$ action.
\begin{example}
  Suppose a person A has 3 pants and 2 shirts. How many different pairs of a pant and a shirt can A dress up with?
  \Tree[.2S $S_1$ $S_2$ ]
  \Tree[.3P $P_1$ $P_2$ $P_3$ ]
  \\$S_1P_1, S_1P_2, S_1P_3, S_2P_1, S_2P_2, S_2P_3$ are all of the possible pairs. There are $3\times2=6$ pairs.
\end{example}
\begin{example}
  License plates in Arizona consist of three digits followed by three letters. How many different license plates are possible?\\
  Solution: $10\times10\times10\times26\times26\times26=17,576,000$ combinations.
\end{example}
\subsection{Other counting rules}
\subsubsection{Permutations}
A permutation of $r$ objects form a collection of $m$ objects in any \textit{ordered} arrangement of $r$ distinct objects from the $m$ objects.
\[mPr = \frac{m!}{(m-r)!}\]
\begin{example}
  How many permutations are there for the word "ROSE"?\\
  Solution: There are $4!=24$ permutations.\\
  What about for "TOO"? Well, it depends if 'O' and 'O' are distinct. If they're distinct, it would be $3!=6$ permutations. But, if they aren't distinct, then it would be $\frac{3!}{2!} = 3$ permutations.\\
  For example, find the number of distinct permutations of the letters "BERKELEY", noting that some letters are identical.\\
  Solution: $\frac{8!}{3!}=8\times7\times6\times5\times4$
\end{example}
\subsubsection{Combinations}
Combinations of $r$ objects from a collection of $m$ objects in any unordered arrangement.
\[mCr = \binom{m}{r}=\frac{m!}{r!(m-r)!}\]
\begin{theorem}
  Binomial expansion: For any integer $n\geq 0,$
  \[(x+y)^n=\sum_{i=0}^{n}\binom{n}{i}x^{n-i}y^i\]
\end{theorem}
\begin{example}
  A committee of 3 persons is to be constituted from a group of 2 men and 3 women. In how many ways can this be done? How many of these committees would consists of 1 man and 2 women?\\
  Solution: Since the order doesn't matter, combination of 5 different persons taken from 3 at a time,
  \[\binom{5}{3}=\frac{5!}{3!2!}=10.\]
  For 1 man and 2 women,
  \[\binom{2}{1}\times\binom{3}{2}=6.\]
\end{example}
\begin{example}
  From an ordinary deck of 52 cards, 7 are drawn at random, (1) without replacement, and (2) with replacement. What is the probability that at least one of the cards is a king?\\Solution:
  \begin{enumerate}
    \item Let $A$ be the event of interest. Then $\PP(A)=1-\PP(A^c)$. $A^C$ is the event that none of the 7 selected cards is a king.
      \[\PP(A^C)=\frac{\binom{48}{7}}{\binom{52}{7}}\]
      Hence, $\PP(A)=1-\frac{\binom{48}{7}}{\binom{52}{7}}$.
    \item $\PP(A^c)=\frac{48^7}{52^7}.$ Hence, $\PP(A)=1-\frac{48^7}{52^7}$.
  \end{enumerate}
\end{example}
\begin{example}
  You have 100 towels: 40 are yellow, 60 are white.
  \begin{enumerate}
    \item What is the probability of choosing 5 towels where 3 are white and 2 are yellow?
    \item What is the probability that we will have at least 3 white towels?
    \item What is the probability that we will have at most 3 white towels?
  \end{enumerate}
  Solution: Sample space will contain all of the possibilities of selecting the 5 towels. $|\Omega|=$ the number of elements in $\Omega = \binom{100}{5}$.
  \begin{enumerate}
    \item Let $A$ be the event of choosing 3 white towels and 2 yellow towels.
      \[(A)=\binom{60}{3}\times\binom{40}{2}\]
      Now, since the sample space is finite, classical probability model applies here:
      \[\PP(A)=\frac{\binom{60}{3}\binom{40}{2}}{\binom{100}{5}}\]
    \item $\PP(B)=\PP(3W+2Y)+\PP(4W+1Y)+\PP(5W+0Y)=\frac{\binom{60}{3}\binom{40}{2}}{\binom{100}{5}}+\frac{\binom{60}{4}\binom{40}{1}}{\binom{100}{5}}+\frac{\binom{60}{5}\binom{40}{0}}{\binom{100}{5}}$
    \item $\PP(C)=\PP(0W+5Y)+\PP(1W+4Y)+\PP(2W+3Y)+\PP(3W+2Y)$
  \end{enumerate}
\end{example}
\newpage
\section{Conditional Probability and Independence}
\subsection{Conditional Probability}
The probability of $E$ given $F$, or $\PP(E\mid F)=\frac{\text{number of elements in }E\cap F}{\text{number of elements in }F}$.
\begin{example}
  Consider the experiment of tossing a balanced coin three times:
  \[\Omega=\{HHH,HHH,HHT,HTH,THH,HTT,THT,TTH,TTT\}\]
  \[|\Omega|=2^3=8\]
  Let $E$ be the event that at least two heads appear. Let $F$ be the event that the first coin shows tails. Then,
  \[E=\{HHH,HHT,HTH,THH\}\]
  \[F=\{THH,THT,TTH,TTH\}\]
  \[\PP(E)=\frac{1}{2},\PP(F)=\frac{1}{2},\PP(E\cap F)=\frac{1}{8}\]
  Question: Suppose the first coin shows tails($F$ occurs). What is the probability of the occurrence of event $E$?\\
  The probability of $E$ considering $F$ as the sample space is $\frac{1}{4}$. AKA, $\PP(E\mid F)=\frac{1}{4}$.
\end{example}
\begin{definition}
  Let $(\Omega,\mathcal{F} , \PP)$ be the probability space. Let $E,F\in\mathcal{F}$ be events such that $\PP(F)>0.$ The conditional probability of $E$ given $F$, denoted by $\PP(E\mid F)$ is defined by
  \[\PP(E\mid F)=\frac{\PP(E\cap F)}{\PP(F)}\]
\end{definition}
\begin{example}
  Roll two fair dices. What is the probability that the numbers on the dice add up to 8, given that the dice show different numbers?\\
  Solution: Let $A$ be the event that numbers on dice add up to 8. Let $B$ be the event that the numbers on dice show different numbers. So, $\Omega=\{(i,j):1\leq i,h\leq6\}, |\Omega|=6^2=36$
  \[A=\{(2,6),(6,2),(3,5),(5,3),(4,4)\},|A|=5\]
  \[B=\{(i,j):1\leq i\leq6,1\leq j\leq6,i\neq j\},|B|=30\]
  \[A\cap B=\{(2,6),(6,2),(3,5),(5,3)\},\PP(A\mid B)=\frac{\PP(A\cap B)}{\PP(B)}\implies\frac{\frac{4}{36}}{\frac{30}{36}}=\frac{4}{30}\]
\end{example}
\subsection{General Multiplication Rule}
Proposition: Let $A$ and $B$ be events with $\PP(A)>0.$ Then, \[\PP(A\cap B)=\PP(A)\times \PP(B\mid A).\]
More generally, if $A_1, A_2, \dots, A_N$ are events with $\PP(A_1\cap\dots\cap A_{N-1})>0.$
\[\PP(\bigcap_{n=1}^N A_n)=\PP(A_1)\times\PP(A_2\mid A_1)\times\dots\times\PP(A_N\mid A_1\cap\dots\cap A_{N-1})\]
\begin{example}
  For the 107th Congress, 18.7\% of members are senators and 50\% of the senators are democrats. What is the probability that a randomly selected member of the 107th Congress is a democratic senator?
  \\Solution: Let $D$ be the event that the member selected is a democrat. Let $S$ be the event that the member selected is a senator.
  \[\PP(S)=0.187, \PP(D\mid S)=0.5\]
  By the multiplication rule,
  \[\PP(D\cap S)=\PP(S)\times \PP(D\mid S)=0.187\times0.5=0.094\]
\end{example}
\begin{example}
  An urn contains 5 red and 4 blue balls. A total of 4 balls is drawn sequentially (without replacement). What is the probability that colors of balls alternate?\\
  Solution: Let $R_1, B_i$ be events that the $i^{th}$ ball is red, blue respectively. That means we want
  \[\PP(\{R_1\cap B_2\cap R_3\cap B_4\}\cup\{B_1\cap R_2\cap B_3\cap R_4\})=\PP(R_1\cap B_2\cap R_3\cap B_4)+\PP(B_1\cap R_2\cap B_3\cap R_4)\]
  By the general multiplication rule,
  \[\PP(R_1\cap B_2\cap R_3\cap B_4)=\PP(R_1)\times\PP(B_2\mid R_1)\times\PP(R_3\mid R_1\cap B_2)\times\PP(B_4\mid R_1\cap B_2\cap R_3)\]
  \[=\frac{5}{9}\times\frac{4}{8}\times\frac{4}{7}\times\frac{3}{6}\]
  Similarly,
  \[\PP(B_1\cap R_2\cap B_3 \cap R_4)=\frac{4}{9}\times\frac{5}{8}\times\frac{3}{7}\times\frac{4}{6}\]
  Add these two together for the final probability.
\end{example}
\subsection{The Law of Total Probability}
Proposition: Suppose that $A_1, A_2, \dots$ form a partition $(A_i\cap A_j =\phi,i\neq j \& \bigcap_{n\geq1} A_n=\Omega)$ of sample space $\Omega$. Then, for each event $B$,
\[\PP(B)=\sum_{n=1}^\infty \PP(A_n)\PP(B\mid A_n).\]
In particular, if $E$ is an event, then
\[\PP(B)=\PP(E)\PP(B\mid E)+\PP(E^c)\PP(B\mid E^c)\]
for each event $B$.
\begin{example}
  54\% of US men and 33\% of US women believe in aliens. Also, 48\% of US adults are men. What percentage of US adults believe in aliens?\\
  Solution: Assume that a US adult is selected at random and $A$: adult selected believes in aliens, $M:\{$ adult selected is a man $\}$.\\
  $\PP(M)=0.48;\PP(N^c)=0.52;\PP(A\mid M)=0.54; \PP(A\mid M^c)=0.33.$
  Apply the law of total probability:
  \[\PP(A)=\PP(M)\times\PP(A\mid M)+\PP(M^c)\times\PP(A\mid M^c)=0.4308\]
  43.08\% of US adults believe in aliens.
\end{example}
\subsection{Independent Events}
In general, $\PP(A\mid B)\neq\PP(A)$. But, if two events $A$ and $B$ are independent, then the occurrence of $A$ does not affect $B$, or
\[\PP(A\mid B)=\PP(A).\]
\begin{example}
  Consider the experiment of randomly selecting one card from a deck of 52. Let $K:$ event a King is selected. $H:$ event a heart is selected.
  \[\PP(K\mid H)=\frac{1}{13}=0.077\]
  \[\PP(K)=\frac{4}{52}=0.077\]
  Therefore, they are independent.
\end{example}
\begin{definition}
  Let $(\Omega,\mathcal{F}, \PP)$ be a sample space; And $A, B\in \mathcal{F}$ with $\PP(A)>0$. We say that event $B$ is independent of event $A$ if the occurrence of $A$ doesn't affect the probability that event $B$ occurs, i.e.
  \[\PP(B\mid A)=\PP(B), \PP(A)>0.\]
  Two events are also said to be independent events if
  \[\PP(A\cap B) = \PP(A)\times\PP(B).\]
\end{definition}
Mutually exclusive(disjoint) and independence are two different things. Mutually exclusive means two sets do not overlap. Independence means one event happening does not affect the outcome of the other.
\subsection{Bayes' Theorem}
\begin{example}
  One bag contains 3 red and 4 black balls while the other bag contains 5 red and 6 black balls. One ball is drawn at random from one of the bags. What is the probability that the color of the ball is red?
  \\Solution: Let $E_1$ be the event of choosing from bag 1, and $E_2$ be the event of choosing from bag 2, and $A$ be the event of drawing a red ball. Then, $\PP(E_1)=\PP(E_2)=\frac{1}{2}$. Also, $\PP(A\mid E_1)=\frac{3}{7}$, and $\PP(A\mid E_2)=\frac{5}{11}$. Hence, by the law of total probability,
  \[\PP(A)=\sum\PP(E_n)\PP(A\mid E_n)\]
  The probability of drawing a ball from bag 2, being given that the ball is red,
  \[\PP(E_2\mid A)=\frac{\PP(E_2\cap A)}{\PP(A)}=\frac{\PP(E_2)\PP(A\mid E_2)}{\PP(E_1)\PP(A\mid E_1)+\PP(E_2)\PP(A\mid E_2)}\]
\end{example}
\begin{theorem}
  Suppose that $A_1, A_2, \dots$ form a partition of the sample space. Then, for each event $B$,
  \[\PP(A_j\mid B)=\frac{\PP(A_j)\PP(B\mid A_j)}{\sum_{n\geq1} \PP(A_n)\PP(B\mid A_n)}\]
\end{theorem}
\begin{example}
  Machine A, B, and C respectively manufacture 25\%, 35\%, and 40\% of the bolts. Of their outputs, 5, 4, and 2 percent are respectively defective bolts. A bolt is drawn at random from the product and is found to be defective. What is the probability that it is manufactured by machine B?
  \\Solution: Let $E$ be the event that the bolt is defective. And $B_1, B_2, B_3$ be the events that the bolt is manufactured by machines A, B, and C respectively. $\PP(B_1)=0.25, \PP(B_2)=0.35,\PP(B_3)=0.4.$ If we want $\PP(B_2\mid E), $ then $\PP(E\mid B_1)=0.05, \PP(E\mid B_2) = 0.04, \PP(E\mid B_3) = 0.02.$
  Hence, by Bayes's theorem,
  \[\PP(B_2\mid E)=\frac{\PP(B_2)\PP(E\mid B_2)}{\sum_{i=1}^3 \PP(B_i)\PP(E\mid B_i)}\]
\end{example}
\section{Random Variables and their Distribution}
\begin{definition}
  Random Variable: Let $(\Omega,\mathcal{F},\PP)$ be a probability space. A function $x:\Omega\rightarrow R$ is called a random variable.
\end{definition}
\begin{example}
  Toss a fair coin three times. \[\Omega=\{HHH,HHT,HTH,HTT,THH,THT,TTH,TTT\}\]$X:$ number of $H$'s. So, $X(HHH)=3,X(HHT)=2,X(TTT)=0,X:\Omega\rightarrow\{0,1,2,3\},$ so $X$ is a discrete random variable.\\
  Notation: for $x\in \R, \{X=x\}=\{\omega\in\Omega\mid X(\omega)=x\}$
  \[\{X=2\}=\{\omega:x(\omega)=2\}=\{HHT,HTH,THH\}\]
  \[\{X=1\}=\{\omega\in\Omega\mid X (\omega)=1\}=\{HTT,THT,TTH\}\]
  \[\{X=0\}=\{TTT\}; \{X=3\}=\{HHH\}\]
  \[\PP(X=3)=\PP(\omega:X(\omega)=3)=\frac{1}{8}\]
  \[\PP(X=1)=\PP(X=2)=\frac{3}{8}\]
  More formally:
  \[\PP(x=x)=
    \begin{cases}
      \frac{1}{8};x=0\\\frac{1}{8};x=3\\\frac{3}{8};x=1\\\frac{3}{8};x=2\\0\text{ if }x\notin\{0,1,2,3\}
  \end{cases}\]
  Note: $\sum_{x\in\R} \PP(X=x)=1$
\end{example}
\subsection{Discrete Random Variables}
A real-valued function on the sample space of a random experiment whose possible values (range) can be assumed to be a countable set.
\begin{definition}
  Probability Mass Function: Let $X$ be a discrete random variables (r.v.). Then the probability mass function (PMF) of $X$, denoted by $p_X$, is a function
  \[p_X:\R\rightarrow[0,1] \text{ s.t. } p_X(x)=\PP(X=x).\]
\end{definition}
Proposition(Basic properties of a PMF):
\begin{enumerate}
  \item $p_X(x)\geq0\forall x\in\R$
  \item $\sum_{x\in\R}p_X(x)=1$
  \item $\{x\in\R\mid p_X(x)\neq 0\}$ is countable
\end{enumerate}
\begin{example}
  Consider the random experiment of tossing a coin three times. Let $X$ denote the total number of heads obtained in three tosses of the coin. Determine $\PP(1\leq X\leq  3)$.\\
  Solution: Possible values of $X$ are $0,1,2,3.$ Hence, if $x\notin\{0,1,2,3\},(X=x)=(\omega\mid X(\omega)=x)=p$.
  \[\{1\leq x<3\}=\{\omega\in\Omega\mid 1\leq X(\omega)<3\}=\{\omega\mid X(\omega)=1\}\cup\{\omega\mid X(\omega)=2\}=\]\[\{X=1\}\cup\{X=2\}\]
  Hence, $\{1\leq X<3\}$ is the union of two mutually exclusive events, i.e. $\{X=1\}$ and $\{X=2\}.$
  \[\PP(1\leq X<3)=\PP(X=1)+\PP(X=2)=p_X(1)+p_X(2)=\frac{3}{8}+\frac{3}{8}=\frac{3}{4}\]
\end{example}
In general, for a discrete random variable, and $A\subset\R$, $\PP(X\in A)=\sum_{x\in A} p_X(x).$
\subsubsection{Bernoulli random variables}
\begin{enumerate}
  \item An experiment with two outcomes ($\Omega = \{S, F\}$)
  \item Independent trials -- i.e. Bernoulli trials
  \item Fixed probabilities of success -- i.e. $p\in[0,1]$
\end{enumerate}
Let $x$ be the number of success in a single Bernoulli trial. Thus, $x$ is a discrete random variable taking values of $\{0,1\}$ -- i.e. $x(F)=0$ and $x(S)=1$.\\
For PMF,
\[p_x(1)=\PP(x=1)=\PP(S)=p\]
\[p_x(0)=\PP(x=0)=\PP(F)=1-p\]
Hence,
\[p_x=
  \begin{cases}
    p;\text{if }x=1\\1-p;\text{if }x=0\\0;\text{otherwise}
\end{cases}\]
Here, the random variable whose RMF is given by the above piecewise is called a Bernoulli random variable, and it is denote by $X\sim Ber(p)$.
\subsubsection{Binomial random variables}
To define a binomial random variable, we start with three Bernoulli trials, with probability of success $p$. \\
Sample space: $\Omega=\{SSS,SSF,\dots,FFF\}=8$ \\
Let $x$ be the number of success in 3 trials. So, the range of $x=\{0,1,2,3\}$.\\
The probability of the number of successes is as follows:
\[\PP(x=0) = \PP(\{FFF\})=\PP(F)\times\PP(F)\times\PP(F)=(1-p)^3\]
\[\PP(x=1) = \PP(\{SFF,FSF,FFS\})=\PP(\{SFF\})+\PP(\{FSF\})+\PP(\{FFS\})=\]\[3(\PP(S)\PP(F)\PP(F))=3p(1-p)^2\]
Now, let's find the probability of $x$ successes in $n$ Bernoulli trials. Clearly, $x$ successes can be obtained in \[nCx = \frac{n!}{x!(n-x)!}\text{ ways.}\]
In each of these ways, the probability of $x$ successes is
\[=\PP(x)\times\PP(n-x)=p^x\times(1-p)^{n-x}.\]
Thus, the probability of x successes in $n$ Bernoulli trials is
\[\binom{n}{x}p^x(1-p)^{n-x};x=0,1,\dots,n\]

\begin{example}
  If a fair coin is tossed 10 times, find the probability of
  \begin{enumerate}
    \item exactly six heads
    \item at least six heads
    \item at most six heads
  \end{enumerate}
  Solution: Here, Bernoulli trials are repeated tosses. $X$: number of heads in an experiment of 10 trials.
  $\therefore X$ has the binomial with $n=10, p = \frac{1}{2}. \therefore PMF$ of $X$ is $p_X(x)=\binom{n}{x}p^x(1-p)^{n-x}\implies P(X=x)p_X(x)=\binom{10}{x}(\frac{1}{2})^{10}$
  \begin{enumerate}
    \item $\PP(X=6)=\PP\{\omega:X(\omega)=6\}=p_X(6)\implies\PP(X=6)=\binom{10}{6}(\frac{1}{2})^{10}$
    \item $\PP(X\geq 6)=\PP\{\omega:X(\omega)\geq6\}=\sum_{x=6}^{10} p_X(x)=\sum_{x=6}^{10} \binom{10}{x}(\frac{1}{2})^{10}$
    \item $\PP(X\leq 6)=\sum_{x=0}^6 p_X(x)=\sum_{x=0}^6 \binom{10}{x}(\frac{1}{2})^{10}$
  \end{enumerate}
\end{example}
\subsubsection{Poisson random variables}
A discrete random variable $X$ taking values in $\{0,1,\dots\}$ is called a Poisson random variable $(X \sim Pois(\lambda))$ if its PMF has the form
\[p_X(x)=\PP(X=x)=
  \begin{cases}
    e^{-\lambda}\frac{\lambda^x}{x!}, x=0,1,\dots\\0,\text{ otherwise}
\end{cases}\]
Here, the parameter $\lambda$ of Poisson random variable represents its average value.
\begin{example}
  Let the number of patients arriving in an emergency room from 6 to 7 pm have a Poisson distribution with parameter $\lambda=6.9$. Determine the probability that, on a given day, the number of patients arriving at the emergency room between 6 to 7 will be
  \begin{enumerate}
    \item exactly four
    \item at least two
    \item between four and ten, inclusive
  \end{enumerate}
  Solution: The PMF of $X$ is $p_X(x)=e^{-6.9}\frac{6.9^x}{x!}, x=0,1,2,\dots$.
  \begin{enumerate}
    \item $\PP(X=4)=e^{-6.9}\frac{6.9^4}{4!}=0.095$
    \item $\PP(X\geq2)=1-\PP(X<2)=1-\sum_{x<2} p_X(x)=1-p_X(0)-p_X(1)=0.992$
    \item $\PP(4\leq X\leq10)=\sum_{4\leq x\leq10} p_X(x)=0.821$
  \end{enumerate}
\end{example}
\subsubsection{Geometric random variable}
Consider repeated Bernoulli trials with success probability $p$. Let $X$ be the number of trials up to and including the first success. Then PMF of the random variable $X$ is
\[p_X(x)=\PP\{X=x\}=p(1-p)^{x-1}; x=1, 2, \dots.\]
\begin{definition}
  A random variable $X$ taking values $\{1, 2, \dots\}$ is called a geometric random variable if its PMF is given by
  \[p_X(x)=
    \begin{cases}
      p(1-p)^{x-1},x=1, 2, \dots\\0,\text{ otherwise}
  \end{cases}\]
\end{definition}
So, geometric distribution describes the waiting time for the first success.\\
Proposition: Let $X\sim g(p).$ Then, $\PP(X>n)=(1-p)^n.$ So, the probability of the event that first first success takes at least $(n+1)$ trials in $(10p)^n$.
\subsubsection{Lack-of-memory property}
A random variable is said to satisfy the lack-of-memory property if
\[\PP(X=n+k\mid X>n)=\PP(X=k);n,k\in\N.\]
Proposition: Geometric random variable is the only discrete random variable
that has the lack-of-memory property.
\subsubsection{Negative Binomial random variable}
Consider repeated Bernoulli trials with success probability $p$. Let $X$ denote the number of trials up to and including the $r^{th}$ success. Then,
\[\PP(X=x)=\binom{x-1}{r-1}p^r(1-p)^{x-r};x=r,r+1,\dots\]
\subsection{Functions of discrete random variables}
\begin{example}
  Let $X$ be a discrete random variable. Then $Y=X^2$ is also a discrete random variable. The PMF of $X$ is
  \begin{center}
    \begin{tabular}{||c c c c||}
      \hline
      $x$&-1&0&1 \\ [0.5ex]
      \hline\hline
      $p_X(x)$&0.1&0.7&0.2 \\
      \hline
    \end{tabular}
  \end{center}
  Then, find the PMF of $y$. For this, possible values of $y$ are $\{0,1\}$, or $\PP(y=0)$ \& $\PP(y=1)$.
  \[\PP(y=0)=\PP(x^2=0)=\PP(x=0)=0.7\]
  \[\PP(y=1)=\PP(x^2=1)=\PP(X\in\{-1,1\})=p_X(-1)+p_X(1)=0.1+0.2=0.3\]
  \[\PP(y=k)=0\text{ if } k\neq0,1\]
  \newpage
  The PMF of $y$ is
  \begin{center}
    \begin{tabular}{||c c c||}
      \hline
      $y$&0&1 \\ [0.5ex]
      \hline\hline
      $p_X(x)$&0.7&0.3 \\
      \hline
    \end{tabular}
  \end{center}
\end{example}
Proposition: Let $x$ be a discrete random variable and let $g$ be a real-valued function defined on the range of $X$. Then, the PMF of $y=g(X)$ is
\[p_Y(y)=\PP(Y=y)=\sum_{x\in g^{-1}\{y\}} p_X(x)=\PP(g(X)=y)=\PP(X\in g^{-1}(\{y\}))\]
where $g^{-1}\{y\}=\{x\mid g(x)=y\};$ preimage of $y$ under map $g$.\\
Note: If $g$ is one-to-one, then $p_Y(y)=p_X(x)=p_X(g^{-1}(y)).$
\begin{example}
  Let $X\sim g(p)$ (geometric random variable), $p_X(x)=p(1-p)^{x-1};x=1, 2, \dots$. Find the distribution of $y=x^2$.\\
  Solution: The range of $X$ is $\{1, 2, \dots\}$, hence the range of $Y$ is $\{1, 4, 9, \dots\}$. It is clear that $g=x^2$ is one-to-one on the range of $X$. Then, $y=g(x)\implies x=g^{-1}(y)=\sqrt{y}$. Hence, for $y\in\{1, 4, 9, \dots\}$, we have $p_X(g^{-1}(y))=p_X(\sqrt{y})=p(1-p)^{\sqrt{y}-1}.$
\end{example}
\section{Jointly discrete random variables}
Let $X$ and $Y$ be two discrete random variables defined on the same probability space $\Omega$. Then, the joint probability mass function (joint PMF) of $X$ and $Y$ (denoted $p_{X,Y}$) is a function
\[p_{X,Y}:\R^2\rightarrow [0,1]\]
such that $p_{X,Y}(x,y)=\PP(X=x,Y=y).$
\[\{X=x,Y=y\}=\{X=x\}\cap\{Y=y\}, \{\omega\in\Omega\mid X(\omega)=x,Y(\omega)=y\}.\]
Note: If any one of the event $\{X=x\}$ and $\{Y=y\}$ doesn't occur; then $\{X=x,Y=y\}=\phi$. As with the single variable case, we should have that,
\begin{enumerate}
  \item $p_{X,Y}(x,y)\geq 0$
  \item $\sum_{X,Y} p_{X,Y}(x,y)=1$
  \item $\{(x,y)\in\R^2\mid p_{X,Y}(x,y)\neq0\}$ is a countable set
\end{enumerate}
\begin{example}
  Consider 50 homes currently for sale, where the number of bedrooms and number of bathrooms are given in the table: $X\sim$ number of bedrooms, $Y\sim$ number of bathrooms:
  \begin{center}
    \begin{tabular}{||c c c c||}
      \hline
      $X|Y$&1&2&3 \\ [0.5ex]
      \hline\hline
      2&10&7&0\\
      \hline
      3&5&10&2\\
      \hline
      4&3&8&5\\
      \hline
    \end{tabular}
  \end{center}
  Suppose that one of these 50 homes is selected at random. Find $p_{X,Y}(2,1)$ and $\PP(X=2).$\\
  Solution: $p_{X,Y}(2, 1)=\PP(X=2,Y=1\}=\frac{10}{50}.$ \\$\PP\{X=2\}=\PP(X=2,Y=1\}+\PP\{X=2,Y=2\}+\PP\{X=2,Y=3\}=\frac{10}{50}+\frac{7}{50}+\frac{0}{50}=\frac{17}{50}$.
\end{example}
Proposition: Let $X,Y$ be discrete random variables. Then, \[p_X(x)=\sum_y p_{X,Y}(x,y),x\in\R\]
and
\[p_Y(y)=\sum_x p_{X,Y}(x,y),y\in\R.\]
\begin{example}
  Let the joint PMF of $X$ and $Y$ be given by
  \[p_{X,Y} (x,y) =
    \begin{cases}
      p^2(1-p)^{x+y-2};x,y\in\N\\=0;o/w
  \end{cases} \]
  Find and identify the marginal PMFs of $X$ and $Y$.\\
  Solution:
  \[p_X(x)=\sum_y p_{X,Y} (x,y)=\sum_y p^2(1-p)^{x+y-2}=p^2\sum_{y=1}^\infty (1-p)^{x+y-2}=\]
  \[p^2(1-p)^{x-2}\sum_{y=1}^\infty (1-p)^y=p^2(1-p)^{x-2}\frac{1-p}{1-(1-p)}=p(1-p)^{x-1}\implies\]
  \[p_X(x) =
    \begin{cases}
      p(1-p)^{x-1};x,y\in\N\\=0;o/w
  \end{cases}\]
  $X\sim g(p);$ geometric random variable with parameter $p$. Similarly, $Y\sim g(p).$
\end{example}
\subsection{Fundamental Probability Formula}
Remember $p_{X,Y}(x,y)=\PP\{X=x,Y=y\}=\PP\{X\in A,Y\in B\}\implies A=\{x\}, B=\{y\}.$ For any subset $A\subset\R^2$, we have
\[\PP((X,Y)\in A)=\mathop{\sum\sum}\limits_{(x,y)\in A}p_{X,Y}(x,y).\]
\begin{example}
  Suppose that one of the 50 homes is selected at random. $X$ is the number of bedrooms, $Y$ is the number of bathrooms.
  \begin{tabular}{||c c c c||}
    \hline
    $X|Y$&1&2&3 \\ [0.5ex]
    \hline\hline
    2&10&7&0\\
    \hline
    3&5&10&2\\
    \hline
    4&3&8&5\\
    \hline
  \end{tabular}
  Determine the probability that the home obtained
  \begin{enumerate}
    \item has the same number of bedrooms and bathrooms
    \item has more bedrooms than bathrooms.
  \end{enumerate}
  Solution:
  \begin{enumerate}
    \item $\PP(X=Y)=\mathop{\sum\sum}\limits_{x=y}p_{X,Y}(x,y), A=\{(x,y)\in\R^2\mid x=y\}$\\
      $=p_{X,Y}(3,3)+p_{X,Y}(2,2)=\frac{2}{50}+\frac{7}{50}=\frac{9}{50}.$
    \item $\PP(X>Y)=\mathop{\sum\sum}\limits_{x>y}p_{X,Y}(x,y), A=\{(x,y)\in\R^2\mid x>y\}$\\
      $=p_{x,y}(2,1)+p_{x,y}(3,1)+p_{x,y}(3,2)+\dots$
  \end{enumerate}
\end{example}
\setcounter{subsection}{2}
\subsection{Conditional Probability Mass Function}
Recall $\PP(F\mid E)=\frac{\PP(F\cap E)}{\PP(E)}$. Assume $X,Y$ are two discrete random variables on the same probability space $\Omega$. Then, for $E=\{X=x\}, F=\{Y=y\}$,
\[\PP(Y=y\mid X=x)=\frac{\PP(X=x,Y=y)}{\PP(X=x)}=\frac{p_{X,Y}(x,y)}{p_X(x)},\]
hence, if $p_X(x)>0$, the conditional probability mass function of $Y$ given $X=x$, denoted by $p_{y\mid x}$,
\[p_{y\mid x}=\frac{p_{X,Y}(x,y)}{p_X(x)},y\in \R.\]
In fact, $p_{y\mid x}$ is a probability mass function.
\begin{example}
  Let $X,Y$ denote the lifetimes of the two components observed at discrete timed units. The joint PMF of $X$ and $Y$ is
  \[p_{X,Y}(x,y)=p^2(1-p)^{x+y-2};x,y\in\N.\]
  Find the conditional PMF of $Y$ given $X=x$:
  \[p_{Y\mid X}(y\mid x)=\frac{p_{X,Y}(x,y)}{p_X(x)}=\frac{p^2(1-p)^{x+y-2}}{p(1-p)^{x-1}}=p(1-p)^{y-1}.\]
  So, $p_{Y\mid X} \sim g(p)$ -- geometric.
\end{example}
Note: $\PP(Y\in A\mid X=x)=\sum_{y\in A} p_{Y\mid X}(y\mid x)$, for any subset $A\subset \R$.
\begin{example}
  Suppose that one of the 50 homes from Example 6.3 is selected at random. Given that a randomly selected home has 3 bedrooms, what the the probability that it has almost three bathrooms? (See tables from previous examples).\\
  Solution: We want $\PP(Y\leq3\mid X=3)=\sum_{y\leq3} p_{Y\mid X}(y\mid3)=p_{Y\mid X}(1\mid3)+p_{Y\mid X}(2\mid3)+p_{Y\mid X}(3\mid3)=\frac{5}{50}+\frac{10}{50}+\frac{2}{50}=\frac{17}{50}.$
\end{example}
\subsection{Independent random variables}
Recall: two events $E,F$ are independent if
\[\PP(E\mid F)=\PP(E);\PP(F)>0.\]
\[\PP(E\cap F)=\PP(E)\cdot\PP(F).\]
In the context of random variables, events are considered (for example) $\{X=x\}.$
\begin{definition}
  Two random variables $X,Y$ defined on the same probability space $\Omega$ are said to be \textbf{independent} if
  \[\PP(X\in A,Y\in B)=\PP(X\in A)\cdot\PP(Y\in B),\text{ for any subsets } A,B.\]
\end{definition}
Proposition: If $X,Y$ are independent random variables, then $g(X),h(Y)$ are also independent; here, $g,h$ are some real-valued functions.\\
Proof: $\PP(g(X)\in A,h(Y)\in B)=\PP(X\in g^{-1}(A),Y\in h^{-1}(B))=\PP(X\in g^{-1}(A))\cdot\PP(Y\in h^{-1}(B))=\PP(g(X)\in A)\cdot\PP(h(Y)\in B).$ Hence $g(X),h(Y)$ are independent.
\begin{example}
  If $X,Y$ are independent, and
  \[z:=x^4+e^X+\sin{X};\]
  \[s:=log|Y|+e^Y\cos{Y}+Y^{175}+3;\]
  $\implies$ $z$ and $s$ are also independent.
\end{example}
Proposition: If $X,Y$ are two discrete random variables on the same probability space, then $X,Y$ are independent if and only if
\[p_{X,Y}(x,y)=p_X(x)\cdot p_Y(y), \text{ for all }x,y\in \R.\]
Proof: Suppose $X,Y$ are independent. We consider $A=\{x\},B=\{y\};x,y\in\R.$
So, $p_{X,Y}(x,y)=\PP(X=x,Y=y)=\PP(X\in A,Y\in B)=\PP(X\in A)\cdot \PP(Y\in B)=\PP(X=x)\cdot\PP(Y=y)=p_X(x)\cdotp_Y(y)=p_{X}(x)\cdotp_Y(y).$
\begin{example}
  Let $X,Y$ be two discrete random variables whose joint PMF is given by
  \[p_{X,Y}(x,y)=p^2(1-p)^{x+y-2};x,y\in\N,0<p<1.\]
  Determine whether $X,Y$ are independent random variables.\\
  Solution: Check if
  \[p_X(x)=
    \begin{cases}
      p(1-p)^{x-1},\text{ if }x\in\N \\ 0, o/w
  \end{cases}, (p_X(x)=\sum_y p_{X,Y}(x,y))\]
  Then do the same for $Y$. This result in a $p_{X,Y}(x,y)=p_X(x)p_Y(y)\implies X,Y$ are independent random variables.
\end{example}
In short, if $X,Y$ are independent random variables, then \[p_{Y\mid X}(y\mid x)=\frac{p_{X,Y}(x,y)}{p_X(x)}=\frac{p_X(x)p_Y(y)}{p_X(x)}=p_Y(y).\]
\subsection{Functions of Two or More Discrete Random Variables}
We want to study the distribution of $z=g(X,Y);$ for given random variables $X,Y$. For example: $g(x,y)=x+y$, or $g(x,y)=xy$, or $g(x,y)=min(x,y)$.\\
Proposition: Let $z=g(X,Y).$ Then, the PMF of the random variable $z$ is
\[p_Z(z)=\mathop{\sum\sum}\limits_{(x,y)\in g^{-1}(\{z\})}p_{X,Y}(x,y);g^{-1}(\{z\})=\{(x,y)\mid g(x,y)=z\}.\]
Proof:
\[p_Z(z)=\PP(Z=z)=\PP(g(X,Y)=z)=\PP((X,Y)\in g^{-1}(\{z\}))=\mathop{\sum\sum}\limits_{(x,y)\in g^{-1}(\{z\})}p_{X,Y}(x,y)\]
\begin{example}
  Let $X,Y\sim g(p)$: $X,Y$ are independent. Determine and identify the PMF of the random variable $z=X+Y$.\\
  Solution: Since $X,Y$ are independent random variables, the joint PMF of $X,Y$ is
  \[p_{X,Y}(x,y)=
    \begin{cases}
      p^2(1-p)^{x+y-2};x,y\in\N \\ 0; o/w
  \end{cases}.\]
  Range of $z=\{2, 3, \dots\}$. From the previous proposition, $p_Z(z)=p_{X+Y}(z)=\mathop{\sum\sum}\limits_{x+y=z}p_{X,Y}(x,y).$
  \[=\sum_{x=1}^{z-1}\sum_{y=z-x}p_{X,Y}(x,y)=\sum_{x=1}^{z-1}\sum_{y=z-x} p^2(1-p)^{x+y-2}=\sum_{x=1}^{z-1} p^2(1-p)^{z-2}\]
  \[=p^2(1-p)^{z-2}\sum_{x=1}^{z-1}1=(z-1)p^2(1-p)^{z-2};z=2, 3,\dots\]
  $Z\sim NB(r,p), r=2\implies X+Y\sim NB(2,p)$, if $X,Y$ are independent geometric random variables.
\end{example}
\begin{example}
  $X,Y$ are two independent geometric random variables with parameter $p$. $X,Y\sim g(p)$. $z=min\{X,Y\}.$ Find the PMF of $Z$.
  \\Solution: Range of $Z$ is the set of all positive integers, $\N$. By the proposition,
  \[p_Z(z)=\mathop{\sum\sum}\limits_{min\{X,Y\}=Z}p_{X,Y}(x,y)=p_{X,Y}(z,z)+\sum_{y=z+1}^\infty p_{X,Y}(z,y)+\sum_{z=z+1}^\infty p_{X,Y}(x,z)\]
  \[=p^2(1-p)^{2z-2}+\sum_{y=z+1}^{\infty}p^2(1-p)^{z+y-2}+\sum_{x=z+1}^\infty p^2(1-p)^{x+z-2}\]
  \[=p^2(1-p)^{2z-2}+2p^2(1-p)^{z-2}\sum_{\omega=z+1}^\infty (1-p)^\omega=p^2(1-p)^{2z-2}+2p^2(1-p)^{z-2}\frac{(1-p)^{z+1}}{1-(1-p)}\]
  \[=(2p-p^2)(1-(2p-p^2))^{z-1}, \text{ if }z\in\N=q(1-q)^x\implies \]\[z=min\{X,Y\}\sim \text{geometric}(q), \text{ where }q=2p-p^2;X\sim g(p), p(1-p)^{x-1}.\]
  \\Second method: To find the distribution of $z=min\{X,Y\}$, we note $Z>z$ if and only if $X>Z$ and $Y>Z$.
  \[\implies\PP(Z>z)=\PP(X>z)\PP(Y>z)=(1-p)^z(1-p)^z=(1-p)^{2z}=\lfloor(1-p)^2\rfloor^z\]\[=(1-(2p-p^2)^z)\implies Z\sim g(2p-p^2).\]
\end{example}
Let $X,Y$ be two discrete random variables with join PMF $p_{X,Y}(x,y)$. Set $Z=X+Y$. Then,
\[p_Z(z)=\mathop{\sum\sum}\limits_{x+y=z} p_{X,Y}(x,y)=\sum_x\sum_{y=z-x} p_{X,Y}(x,y)=\sum_x p_{X,Y}(x,z-x),z\in\R\]
\[\implies p_Z(z)=\sum_x p_{X,Y}(x,z-x).\]
In particular, if $X,Y$ are two independent random variables, then using $p_{X,Y}(x,y)=p_X(x)p_Y(y), p_Z(z)=\sum_x p_X(x)p_Y(z-x).$
\begin{example}
  \begin{enumerate}
    \item If $X,Y$ are two independent variables, $X\sim P(\lambda)$, $Y\sim P(\mu)$. Let $z=X+Y$. Find the PMF of $z$. Claim: $z\sim P(\lambda +\mu)$.\\
      We start: $p_Z(z)=\PP(Z=z)=\sum_x p_X(x)p_Y(z-x)=\sum_{x=0}e^{-\lambda}\frac{\lambda^x}{x!}e^{-\mu}\frac{\mu^{z-x}}{(z-x)!}; z-x=y\geq0, z\geq x\geq0=e^{-\lambda-\mu}\sum_{x=0}^z \frac{\lambda^x}{x!}\frac{\mu^{z-x}}{(z-x)!}=\frac{e^{-\lambda-\mu}}{z!}\sum_{x=0}^z\binom{z}{x}\lambda^x\mu^{z-x}=\frac{e^{-\lambda-\mu}}{z!}(\lambda+\mu)^z.$\\
      $\implies z\sim P(\lambda+\mu).$\\
      Note: $X\sim B(n_1,p),Y\sim B(n_2,p)., X+Y\sim B(n_1+n_2,p).$
  \end{enumerate}
\end{example}
\section{Expected Value of Discrete Random Variables}
\begin{definition}
  The \textbf{expected value} of a discrete random variable, denoted by $\E[X]=\sum_{x} xp_X(x)=\sum_x x\PP(X=x).$ This definition makes sense if the right side is absolutely summable, i.e. $\sum_x p_X(x)<\infty$. We say that $X$ has a finite expectation.
\end{definition}
\begin{example} Following are some examples of this concept:
  \begin{enumerate}
    \item Let $X\sim$ Bernoulli$(p)$, then $\E[X]=1\cdot p+0\cdot(1-p)=p.$
    \item Let $X\sim B(n,p);X=\{0,\dots,\}$. $\E[X]=\sum_x xp_X(x)=\sum_{x=0}^{n} x\binom{n}{x}p^x(1-p)^{n-x}=\sum_{x=1}^n \frac{n!}{(x-1)!(n-x)!}p^x(1-p)^{n-x}=np\sum_{x=1}^n \frac{(n-1)!}{(x-1)!(n-x)!}p^{x-1}(1-p)^{n-x}=np\sum_{x=1}^n \binom{n-1}{x-1}p^{x-1}(1-p)^{n-x}=np\sum_{j=1}^{n-1} \binom{n-1}{j}p^j(1-p)^{(n-1-j)}=np.$
    \item Let $X\sim$ Poisson$(\lambda)$: $\E[X]=\sum_x xp_X(x)=\sum_{x=0}^\infty xe^{-\lambda}\frac{\lambda^x}{x!}=e^{-\lambda}\sum_{x=1}^\infty \frac{\lambda^x}{(x-1)!}=\lambda e^{-\lambda}\sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!}=\lambda e^{-\lambda}\sum_{k=0}^\infty \frac{\lambda^k}{k!}=\lambda e^{-\lambda}\cdot e^\lambda=\lambda$. So, $\E[X]=\lambda.$
  \end{enumerate}
\end{example}
In general, if $g$ is a real valued function defined on the range of random variable $X$, then $\E[g(X)]=\sum_x g(x)p_X(x).$ In particular, if $g(x)=x$,
\[\E[X]=\sum_x xp_X(x)\]
\[\E[g(X,Y)]=\sum_x \sum_yg(x,y)p_{X,Y}(x,y),\text{ provided }\sum_{x,y}(g(x,y)\mid p_{X,Y}(x,y))<\infty.\]
\subsection{Fundamental Properties of Expected Value}
\begin{enumerate}
  \item If $X=$ constant $c$,
    \[\E[X]=\E[c]=\sum_x c\cdot p_X(x)=c\sum_x p_X(x)=c.\]
  \item If $c$ is a constant, then
    \[\E[cX]=c\cdot \E[X].\]
  \item $\E[X+Y]=\E[X]+\E[Y].$
  \item $\E[a+bX] = a + b\E[X]$, where $a,b\in\R$ and $X$ is a discrete random variable.
  \item $\E[aX+bY]=a\E[X]+b\E[Y];a,b\in\R.$
  \item If $X\leq Y$ then $\E[X]\leq\E[Y].$
  \item If $X,Y$ are independent random variables, then
    \[\E[XY]=\E[X]\cdot\E[Y].\]
    In general, if $X_1,\dots,X_n$ are independent, then
    \[\E[\prod_{i=1}^n X_i]=\prod_{i=1}^n \E[X_i]\]
    and always (i.e. independent or not)
    \[\E[\sum_{i=1}^n X_i]=\sum_{i=1}^n \E[X_i].\]
\end{enumerate}
\begin{example} Consider the following examples:
  \begin{enumerate}
    \item $S=X+\sin(Y)+e^Z$
      \[\E[S]=\E[X]+\E[\sin(Y)]+\E[e^Z].\]
    \item If $X\sim Bin(n,p)$, then $\E[X]=np.$ If $Y_i$ are Bernoulli random variables with parameter p, then $X=\sum_{i=1}^n Y_i.$
      \[\E[X]=\E[\sum_{i=1}^n Y_i]=\sum_{i=1}^n \E[Y_i]=\sum_{i=1}^n p = np.\]
    \item Let $X\sim Bin(n,p)$. Then,
      \[\E[e^X]=\sum_{x=0}^n e^xp_X(x)=\sum_{x=0}^n e^x\binom{n}{x}p^x(1-p)^{n-x}\]\[=\sum_{x=0}^n (ep)^x\binom{n}{x}p^x(1-p)^{n-x}=(ep+1-p)^n\]
  \end{enumerate}
\end{example}
Recall that for a random variable $X$,
\[\E[X]=\sum_x x\cdot p_X(x).\]
To develop this definition, the expectation is interpreted a long-run average value of the random variable in repeated trials. That is, for a random variable $X$, let $X_1,\dots,X_n$ are $n$ observed values of $X$, then
\[\frac{X_1+\dots+X_n}{n}\approx \E[X],\text{ for large }n.\]
Here,
\[\E[X]\approx\frac{X_1+\dots+X_n}{n}=\frac{1}{n}\sum_{k=1}^m n(\{X=x_k\})x_k=\sum_{x=1}^n x_k\frac{n(\{X=x_k\})}{n}=\sum_{k=1}^n x_k\PP(X=x_k)\]
\[=\sum_{k=1}^n x_kp_X(x_k);\frac{n(E)}{n}\approx\PP(E)=\sum_x xp_X(x).\]
\subsubsection{Tail Probabilities}
We can use tail probabilities to obtain expected values: Let $X\geq0$ be an integer-values random variable. Then $X$ has finite expectation (i.e. $\E[X]<\infty$) if and only if $\sum_{n=0}^\infty \PP(X>n)<\infty.$ And here, $\E[X]=\sum_{n=0}^\infty \PP(X>n).$
\begin{example}
  Let $X\sim g(p)$; geometric random variable with parameter $p$. Check that $\E[X]=\sum_x xp_X(x).$\\
  Solution:
  $\PP(X>n)=(1-p)^n$. Hence, $\E[X]=\sum_{n=0}^\infty \PP(X>n)=\sum_{n=0}^\infty (1-p)^n = \frac{1}{1-(1-p)}=\frac{1}{p}.$
\end{example}
\subsection{Mean and Variance}
Recall: For a given function $g$, $\E[g(X)]=\sum_x g(x)p_X(x)$ if $g(x)=x^r$; $r\geq1$ is an integer. Also, $\E[X^r]=\sum_x x^rp_X(x)$; provided $\sum_x |x|^rp_X(x)<\infty.$\\
In this case, $\E[X^r]$ is known as the $r^{th}$ moment of random variable $X$. We let $\mu_X=\E[X]$ be the mean of $X$ (the expectation of $X$).
\begin{definition}
  Let $X$ be a random variable with $\E[X^2]<\infty$. Then the variance of $X$, denoted by $var(X)$, is defined by
  \[var(X)=\E[(X-\mu_X)^2];\mu_X=\E[X].\]
\end{definition}
$var(X)=\E[(X-\mu_X)^2]$ is a measure of variation of possible values of random variable $X$. So, if $var(X)$ is large, then on average, $X$ is far from its mean.\\
Proposition: Let $X$ be a random variable. Then, $var(X)=\E[X^2]-(\E[X])^2;\E[X^2]<\infty.$\\
Summary: Moments of $X$ are $\E[X^r];r\geq1$. $var(X)=\E[X-\E[X]^2]=\E[X^2]-(\E[X])^2.$ As a note, higher moments imply lower moments.
\subsubsection{Properties of Variance}
\begin{enumerate}
  \item $\sigma_X^2=var(X)=\E[(X-\E[X])^2].$
  \item Standard deviation (denoted by $\sigma_X$) $=\sqrt{\sigma_X^2}=\sqrt{var(X)}.$
  \item A random variable $X$ has zero variance if it is a constant random variable. In fact, $var(X)=var(c)=\E[(c-\E[c])^2]=\E[c-c]^2=0$.
  \item $var(cX)=c^2var(X)$ where $c\in\R$ and $var(X)<\infty$, since
    \[var(cX)=\E[(cX-\E[cX])^2]=\E[c^2(X-\E[X])^2]=c^2\E[(X-\E[X])^2]=c^2var(X).\]
  \item $var(c+X)=var(X).$
\end{enumerate}
Consequently, $var(aX+b) = a^2var(X).$
\begin{example}
  Following are some examples:
  \begin{enumerate}
    \item $var(9X+5)=81var(X)$.
    \item $X\sim Ber(p);\E[X]=p.$ $var(X)=\E[X^2]-(\E[X])^2=0^2(1-p)+1^2p-(0\cdot(1-p)+1\cdot p)^2=p-p^2=p(1-p).$
  \end{enumerate}
\end{example}
\subsection{Standardized Random Variable}
Let $X$ be a random variable such that $0<var(X)<\infty.$ Then, the standardized random variable (denoted by $X^*)$ is defined by
\[X^*=\frac{X-\E[X]}{\sqrt{var(X)}}=\frac{X-\mu_X}{\sigma_X}.\]
Note: Mean of a standardized random variable is zero:
\[\E[X^*]=\E[\frac{X-\mu_X}{\sigma_X}]=\frac{1}{\sigma_X}\E[X-\mu_X]=\frac{1}{\sigma_X}(\E[X]-\E[\mu_X])=\frac{1}{\sigma_X}(\mu_X-\mu_X)=0.\]
Also, variance of a standardized random variable $X^*$ is $1$:
\[var(X^*)=var(\frac{X-\mu_X}{\sigma_X})=\frac{1}{\sigma_X^2}var(X-\mu_X)=\frac{1}{\sigma_X^2}var(X)=\frac{1}{\sigma_X^2}\cdot\sigma_X^2=1.\]
Thus, $X^*$ has mean 0 and variance 1, which is why it is called the standardized random variable.\\
Recall that $var(X)=\E[(X-\mu_X)^2]=\sum_x (X-\mu_X)^2p_X(x);\mu_X=\E[X].$\\
\textbf{Proposition:} Chebyshev's Inequality: Let $X$ be a random variable with $var(X)<\infty$. Then, for each $\delta>0$,
\[\PP(|X-\mu_X|\geq\delta)\leq\frac{var(X)}{\delta^2}.\]
Note: Chebyshev's inequality also provides a method for obtaining bounds for probabilites of a random variable based on its mean of variance.
\begin{example}
  Let $X\sim Bin(n,p).$ Then, $\E[X]=np;var(X)=np(1-p).$ Can you bound the probability that $X$ differs from its expected value more than 3 units? (i.e. $\PP(|X-\mu_X|\geq3?)$\\
    Solution: By Chebyshev's inequality, $\PP(|X-\mu_X|\geq3)\leq\frac{var(X)}{3^2}=\frac{var(X)}{9}\implies\PP(|X-\mu_X|\geq3)\leq\frac{np(1-p)}{9}.$
  \end{example}
  \subsection{Covariance and Correlation}
  Recall: $\E[X+Y]=\E[X]+\E[Y]$. $\E[XY]=\E[X]\cdot\E[Y]$ if $X,Y$ are independent. Can you write $var(X+Y)=var(X)+var(Y)?$\\
  We start by studying the variance of the sum of two random variables.
  \[\mu_{X+Y}=\E[X+Y]=\mu_X+\mu_Y=\E[X]+\E[Y].\]
Now, we know $var(X)=\E[(X-\mu_X)^2].$ We consider $((X+Y)-\mu_{X+Y})^2=((X+Y)-(\mu_X+\mu_Y))^2=((X-\mu_X)+Y-\mu_Y))^2=(X-\mu_X)^2+(Y-\mu_Y)^2+2(X-\mu_X)(Y-\mu_Y).$\\
Taking expectations on both sides, we have $\E[(X+Y)-\mu_{X+Y}]^2=\E[(X-\mu_X)^2]+\E[(Y-\mu_Y)^2]+2\E[(X-\mu_X)\cdot(Y-\mu_Y)].$\\
$\implies var(X+Y)=var(X)+var(Y)+2\E[(X-\mu_X)(Y-\mu_Y)]$, therefore no.\\
That extra part at the end, the $2\E[(X-\mu_X)(Y-\mu_Y)]$, is known as the \textbf{covariance}.
\begin{definition}
  Let $X, Y$ be two random variables such that $\sigma_X^2,\sigma_Y^2<\infty.$ The covariance of $X,Y$ (denoted by $cov(X,Y)$) is
  \[cov(X,Y)=\E[(X-\mu_X)(Y-\mu_Y)].\]
  Note: If $X=Y$, $cov(X,Y)=var(X)=\E[(X-\mu_X)^2].$
\end{definition}
\subsubsection{Some Properties of Covariance}
Let $X,Y,Z$ be random variables with finite variables. Then,
\begin{enumerate}
  \item $cov(X,Y)=\E[XY]-\E[X]\cdot\E[Y].$
  \item $cov(X,Y)=cov(Y,X).$
  \item $cov(cX,Y)=c\cdot cov(X,Y).$
  \item $cov(X+Y,Z)=cov(X,Z)+cov(Y,Z).$ \\More generally, $var(\sum_{k=1}^m X_k)=\sum_{k=1}^m var(X_k)+2\mathop{\sum\sum}\limits_{i<j}cov(X_i,Y_i).$
\end{enumerate}
\textbf{Proposition:} $var(X+Y)=var(X)+var(Y)+2cov(X,Y).$
\begin{example}
  As an example, expand $cov(2X+3,-5Y+6)$.
  \[=cov(2X,-5Y)=-10cov(X,Y).\]
\end{example}
\textbf{Important Property:} Let $X,Y$ be a independent random variables. then, $cov(X,Y)=0$. However, $cov(X,Y)=0$ does not imply that $X,Y$ are independent. Using this, however, the $var(X+Y)=var(X)+var(Y).$
\begin{definition}
  Does the correlation coefficient say something about how much and how $X,Y$ are related? Consider standardized random variables, $X^*=\frac{X-\mu_X}{\sigma_X}$ and $Y^*=\frac{Y-\mu_Y}{\sigma_Y}.$ The correlation coefficient between $X,Y$ is
  \[\rho(X,Y)=cov(X^*,Y^*)=\frac{cov(X,Y)}{\sqrt{var(X)}\sqrt{var(Y)}}.\]
\end{definition}
\subsubsection{Properties of Correlation Coefficient}
Let $X,Y$ be random variables with finite non-zero variance. Then,
\begin{enumerate}
  \item $|\rho(X,Y)|\leq1.$
  \item If $X,Y$ are independent, then $\rho(X,Y)=0.$
  \item $\rho(X,Y)=1$ iff $Y=a+bX$, with $a\in\R,b>0$.
  \item $\rho(X,Y)=-1$ iff $Y=a+bX$, with $a\in\R,b<0$.
  \item $\rho(X,Y)>0$ means that $X,Y$ are positively correlated.
  \item $\rho(X,Y)<0$ means that $X,Y$ are negatively correlated.
\end{enumerate}
\begin{example}
  If $var(X)=0.3376, var(Y)=0.56, cov(X,Y)=0.224$, then $\rho(X,Y)=\frac{0.224}{\sqrt{var(X)}\cdot\sqrt{var(Y)}}=0.575\implies X,Y$ are positively correlated.
\end{example}
\subsection{Conditional Expectation}
\[\PP(B)=\sum P(B\mid A_n)\cdot\PP(A_n)\] where $A_n$'s were the partitioned events.
\begin{example}
  Let $N=$ number of customers arriving at a bar on a given day, and $S_i=$ amount of time it takes each customer to be served. We are interested in knowing the total amount of time to be served all customers: $Y_N=\sum_{i=1}^NS_i.$ $S_i$ are identically distributed. $\E[S_i]=\mu.$ $\E[Y_n]=\E[\sum_{i=1}^n S_i]=\E[S_1]+\E[S_2]+\dots+\E[S_n]=n\cdot\mu.$ $\E[Y_N]=\E[\sum_{i=1}^N S_i]=\mu\E[N].$
\end{example}
\[\E[X]=\sum x\cdotp_X(x).\]
\[\E[g(X)]=\sum g(x)p_X(x).\]
\[\E[X+Y]=\sum(X+Y)p_{X,Y}(x,y).\]
Recall that for two random variables $X,Y$, the conditional PMF of $Y$ given $X=x$; $p_{Y\mid X}(y\mid x)=\frac{p_{X,Y}(x,y)}{p_X(x)}, p_X(x)>0.$
\begin{definition}
  Let $X,Y$ be two random variables. If $p_X(x)>0$, we define the conditional expectation of $Y$ given $X=x$ as $\E[Y\mid X=x]=\sum_y y\cdot p_{Y\mid X}(y\mid x).$
\end{definition}
\subsubsection{Properties}
\begin{enumerate}
  \item $\E[cY\mid X=x]=c\E[Y\mid X=x]$.
  \item $\E[Y+Z\mid X=x]=\E[Y\mid X=x]+\E[Z\mid X=x].$
  \item $\E[a+bY\mid X=x]=a+b\E[Y\mid X=x].$
\end{enumerate}
\begin{example}
  Let $X\sim P(\lambda), Y\sim P(\mu).$ Find $\E[X\mid X+Y=Z]$, for each non-negative integer $z$.\\
  Solution: First, we need to find the distribution of $Z=X+Y$.
  \[\therefore X\sim P(\lambda);Y\sim P(\mu); \text{ independent}\]
  \[Z=X+Y\sim P(\lambda+\mu).\]
Now, we want to obtain $\E[X\mid Z=z]=\sum x\cdot p_{X\mid Z}(x\mid z).$ Now, we find conditional PMF of $X$ given $Z=z$. So, $p_{X\mid Z}(z\mid z)=\frac{p_{X,Z}(x,z)}{p_Z(z)}=\frac{P(X=x,Z=z)}{P(Z=z)}=\frac{P(X=x,Y=z-x)}{P(Z=z)}=\frac{P(X=x)P(Y=z-x)}{P(Z=z)}=\frac{\frac{e^{-\lambda}\lambda^x}{x!}\frac{e^{-\lambda}\lambda^{z-x}}{(z-x)!}}{\frac{e^{-(\lambda+\mu)}(\lambda+\mu)^z}{z!}}=\binom{z}{x}\frac{\lambda^x\mu^{z-x}}{(\lambda+\mu)^z}=\binom{z}{x}\frac{\lambda^x}{(\lambda+\mu)^x}\cdot\frac{\mu^{z-x}}{(\lambda+\mu)^{z-x}}=\binom{z}{x}(\frac{\lambda}{\lambda+\mu)})^x(1-\frac{\lambda}{\lambda+\mu})^{z-x}.$\\
So, $X\mid Z=x\sim Bin(z,\frac{\lambda}{\lambda+\mu}).$ We know $\E[Bin(n,p)]=np\implies\E[X\mid Z=z]=z\cdot\frac{\lambda}{\lambda+\mu}=\frac{z\lambda}{\lambda+\mu}.$
\end{example}
We define
\[\psi(X)=\E[Y\mid X]\]
and $\E[Y\mid X]$ is called the conditional expectation of $Y$ given $X$.
\subsection{Law of Total Expectation}
Let $X,Y$ be two discrete random variables with $\E[Y]<\infty$. Then,
\[\E[Y]=\sum_x \E[Y\mid X=x]\cdot p_X(x)=\E[\psi(X)]-\E[\E[Y\mid X]].\]
\[\implies\E[\E[Y\mid X]]=\E[Y].\]
In fact, we know
\[\E[g(X,Y)]=\sum_{x,y} g(x,y)p_{X,Y}(x,y).\]
If we consider $g(x,y)=y$, then $\E[Y]=\mathop{\sum\sum}\limits_{(x,y)}y\cdotp_{X,Y}(x,y)=\mathop{\sum\sum}\limits_{(x,y)}y\cdotp_X{x}\cdotp_{Y\mid X}(y\mid x)=\sum_xp_X{x}(\sum_yy\cdotp_{Y\mid X}(y\mid x)=\sum_xp_X{x}\E[Y\mid X=x]\implies\E[Y]=\sum_x[Y\mid X=x]\cdot p_X(x).$
\begin{example}
  Let $N=$ number of customers arriving at a bank on a given day ($N$ is a random variable). $S_i=$ amount of time it takes each customer to be served. Then, we are interested in knowing the total amount of time to serve all customers, i.e. $Y_N=\sum_{i=1}^N S_i$. Aim: $\E[Y_N].$\\
  Solution: We assume that $N$ is independent to each $S_i$ and $S_i$ are identically distributed. $\E[S_i]=\mu$ for all $i$. We have $\E[Y_N\mid N=0]=\E[Y_0\mid N=0]=\E[0\mid N=0)=0.$ For $n\geq1$, $\E[Y_N\mid N=n]=\E[Y_n\mid N=n]=\E[Y_n](\therefore S_i's$ are independent of $N$)$=\E[\sum_{i=1}^nS_i]=\sum_{i=1}^n\E[S_i]=n\cdot\E[S_i].$. Hence, $\E[Y_N\mid N]=\E[S_1]\cdot N.$ Hence, by the Law of Total Explanation, $\E[Y_N]=\E[\E[Y_N\mid N]]=\E[N\cdot \E[S_i]]=\E[S_i]\E[N]\implies\E[Y_N]=\E[n]\E[S_i].$
\end{example}
\subsection{Conditional Variance}
\[var(X)=\E[(X-\mu_X)^2];\mu_X=\E[X].\]
\[var(Y\mid X=x)=\E[(Y-\E[Y\mid X=x])^2\mid X=x]\]\[=\E[Y^2\mid X=x]-(\E[Y\mid X=x])^2.\]
\section{Continuous Random Variables \& their Distributions}
\subsection{Introducing Continuous Random Variables}
\begin{example}
  Suppose that a number is selected at random from the interval $(0,1)$. Let $X$ denote the number obtained. Find $\PP(X=x)$ for $x\in \R$.\\
  Solution: Here, the sample space $\Omega = (0,1)$. For any event $E$,
  \[\PP(E)=\frac{|E|}{|\Omega|}=|E|;E\subset\Omega.\]
  Random variable is $X(x)=x$ if $x\notin(0,1)\implies\PP\{ X=x\}=0.$ If $x\in(0,1):$ $\{X=x\}=\{\omega\mid X(\omega)=x\}=\{ x\mid X(x)=x\}=\{ x\}.$ Hence, $\PP\{X=x\}=|\{x\}|=0.$ Hence, $\forall x\in\R, \PP\{X=x\}=0.$
\end{example}
\begin{definition}
  A random variable $X$ is called a \textbf{continuous random variable} if $\PP(X=x)=0\;\forall\; x\in\R.$ Note: "Continuous" for these random variables is because the range of such random variables form a continuum of real numbers (e.g. in previous example, the range of $X$ was $(0,1)$).
\end{definition}
If $X$ is a discrete random variable,
\[\PP(X\in A)=\sum_x \PP(X=x).\]
In case of a continuous random variable,
\[\PP(X=x)=0.\]
\subsection{Cumulative Distribution Functions}
\begin{definition}
  Let $X$ be a random variable (discrete/continuous). The\\ \textbf{cumulative distribution function} (CDF) of $X$, denoted $F_X$, is the real-valued $f^n$
  \[F_X:\R\rightarrow[0,1]\;s.t.\; F_X(x)=\PP(X\leq x);\;x\in\R.\]
  Note: CDF applies to any random variable, discrete or continuous.
\end{definition}
\begin{example}
  Toss a coin three times, and let $X$ be the number of heads. Determine the CDF of $X$ and compare it with the PMF. Note, $X$ is a discrete random variable. So,
  \[X(\omega)=
    \begin{cases}
      0;\;\omega=TTT\\1;\;\omega\in\{HTT,THT,TTH\}\\2;\;\omega\in\{HHT,HTH,THH\}\\3;\;\omega=HHH
  \end{cases}.\]
  Solution: The PMF of $X$ is
  \[p_X(x)=\PP(X=x)=
    \begin{cases}
      \frac{1}{8};\;x=0\\\frac{3}{8};\;x=1\\\frac{3}{8};\;x=2\\\frac{1}{8};\;x=3\\0;\;else
  \end{cases}.\]
  Now, for CDF: $F_X(x)=\PP(X\leq x)\;\forall\;x\in\R.$ \\Suppose $x<0$(e.g. $x=-0.7$):
  \[\PP(X\leq x)=0.\]
  Suppose $x=0$:
  \[\PP(X\leq x)=\PP(X\leq0)=\PP(X=0)=\frac{1}{8}.\]
  Suppose $0\leq x<1$:
  \[\PP(X\leq x)=\PP(X=0)=\frac{1}{8}.\]
  Suppose $1\leq x<2$:
  \[\PP(X\leq x)=\PP(X=0)+\PP(X=1)=\frac{1}{8}+\frac{3}{8}=\frac{1}{2}.\]
  Suppose $2\leq x<3$:
  \[\PP(X\leq x)=\PP(X=0)+\PP(X=1)+\PP(X=2)=\frac{7}{8}.\]
  Suppose $x\geq3$:
  \[\PP(X\leq3)=1.\]
  Thus,
  \[F_X(x)=
    \begin{cases}
      0;\;x<0\\ \frac{1}{8};\;0\leq x<1\\\frac{1}{2};\;1\leq x<2\\\frac{7}{8};\;2\leq x<3\\1;\;3\leq x
  \end{cases}.\]
\end{example}
\begin{example}
  Let $X$ denote a number selected at random from $(0,1)$. Find the CDF of $X$.\\
  Solution: The cases of $x<0$ and $x\geq1$ are easy. The only problem is when $0\leq x<1$. To figure this out, $F_X(x)=\PP(X\leq x)=\PP\{\omega: X(\omega)\leq x\}=|(0,x)|=x.$ Therefore,
  \[F_X(x)=
    \begin{cases}
      0;\;x<0\\x;\;0\leq x<1\\1;\;x\geq1
  \end{cases}.\]
\end{example}
Proposition: The CDF $F_X$ of random variable $X$ satisfies:
\begin{enumerate}
  \item $F_X$ is non-decreasing, i.e. if $t_1<t_2,F_X(t_1)\leq F_X(t_2).$
  \item $F_X$ is a right-continuous function, i.e. for every $t\in \R, \; F_X(t^+)=F_X(t)$. Also, for every $t\in\R$, $F_X(t^-)$ exists.
  \item $\lim_{x\to\infty} F_X(x)=1$, and $\lim_{x\to -\infty} F_X(x)=0$.
\end{enumerate}
\subsubsection{Computing Probability using CDF}
Let $X$ be a random variable with CDF $F_X$. Let $a,b\in\R,a<b$.
\begin{enumerate}
  \item $\PP(X\leq a)=F_X(a).$
  \item $\PP(a<X\leq b)=F_X(b)-F_X(a).$
  \item $\PP(a<X\leq b)=F_X(b)-F_X(a);\;a<b.$
  \item $\PP(X<a)=F_X(a^-).$
  \item $\PP(X=a)=F_X(a)-F_X(a^-).$
  \item $\PP(a\leq X\leq b)=F_X(b)-F_X(a^-).$
  \item $\PP(X\geq a) = 1-F(a^-).$
\end{enumerate}
Proposition: A random variable is continuous iff its CDF is an everywhere continuous function.
\subsection{Probability Density Functions}
\begin{definition}
  Let $X$ be a continuous random variable. A non-negative function
  \[f_X:\R\rightarrow[0,\infty)\]
  is said to be a \textbf{probability density function} (PDF) of $X$, if for all $a<b$,
  \[\PP(a\leq X\leq b)=\int_a^b f_X(x) \,dx.\]
\end{definition}
Proposition: Let $X$ be a continuous random variable. If $F_X'(x)$ exists and is continuous, then $X$ has a PDF, and it is given by
\[f_X(x)=
  \begin{cases}
    F_X'(x),\;\text{ if }f_X \text{ is differentiable at } x\\0,\;o/w
\end{cases}.\]
Proposition: A random variable $X$ has a PDF iff there exists a non-negative function $f$ defined on $\R$ such that
\[F_X(x)=\PP(X\leq x)=\int_{-\infty}^x f(t)\,dt,x\in\R.\]
In this case, $f$ is a PDF of $X$ and $f(x)=F_X'(x).$
\subsubsection{Basic Properties of PDF}
Suppose $X$ is a continuous random variable with PDF $f_X$ and CDF $F_X$.
\begin{enumerate}
  \item Since $f_X$ is a density of probability (rather than probability), $f_X$ may be $\geq1$.
  \item $\int_{-\infty}^{\infty}f_X(x)\,dx = 1\;(\therefore \PP(X<\infty)=\int_{-\infty}^{\infty}f_X(x)\,dx=1).$
  \item $\PP(a<X\leq b)=\PP(a\leq X\leq b)=\PP(a\leq X<b).$
\end{enumerate}
\subsubsection{Fundamental Probability Formula}
Proposition: Suppose that $X$ is a continuous random variable with PDF $f_X$. Then, for any subset $A\subset \R$:
\[\PP(X\in A)=\int_A f_X(x)\,dx.\]
For a discrete case, $\PP(X\in A)=\sum_{x\in A} p_X(x).$
\begin{example}
  Let
  \[f_X(x)=
    \begin{cases}
      0;\;x<2\\\frac{c}{x^2};x\geq 2
  \end{cases}.\]
  \begin{enumerate}
    \item Find $c$ such that $f_X(x)$ is PDF.
    \item Compute $\PP(X\geq5).$
    \item Find $\PP(X>0).$
  \end{enumerate}
  Solution:
  \begin{enumerate}
    \item $\int_{-\infty}^{\infty} f_X(x)\,dx\implies\int_{-\infty}^{2} f_X(x)\,dx + \int_{2}^{\infty} f_X(x)\,dx=1\implies0+\int_{2}^{\infty} \frac{c}{x^2}\,dx=1\implies c=2$. So, for $c=2$, $f_X(x)$ is a PDF of random variable $X$.
    \item From FPF, we know $\PP(X\in A)=\int_A f_X(x)\,dx .$ Here, in this case, $A=[5,\infty)\subset\R.$
      \[\PP(X\in[5,\infty))=\PP(X\geq5)=\int_5^\infty f_X(x)\,dx=\int_5^\infty \frac{2}{x^2}\,dx=\frac{2}{5}.\]
      So, $\PP(X\geq5)=\PP(X>5)=\frac{2}{5}\;(\therefore X$ is a continuous random variable).
    \item $\PP(X>0)=\int_0^\infty f_X(x)\,dx=1.$
  \end{enumerate}
\end{example}
\begin{example}
  Let's now find the CDF from the PDF from the previous example. We know $\PP(X\leq x)=f_X(x)=\int_{-\infty}^\infty f_X(x)\,dx.$ So, $F_X(x)=\int_{-\infty}^x f_X(y)\,dy=0,$ if $ x\leq2$.\\
  For $x>2$,
  \[F_X(x)=\int_{-\infty}^x f_X(y)\,dx=\int_2^{x}f_X(y)\,dy=(1-\frac{2}{x}).\]
  So,
  \[F_X(x)=
    \begin{cases}
      0;\;x\leq2\\1-\frac{2}{x};\;x>2
  \end{cases}.\]
  \[f_X(x)=
    \begin{cases}
      0;\;x<2\\\frac{2}{x^2};\;x\geq2
  \end{cases}.\]
\end{example}
\subsection{Uniform and Exponential Random Variables}
\begin{definition}
  A continuous random variable $X$ is called a \textbf{uniform random variable} over $(a,b)$ if its value is equally likely to lie anywhere in this interval. We write \[X\sim U(a,b)\] and its PDF is
  \[f_X(x)=
    \begin{cases}
      0;\;x\notin(a,b)\\\frac{1}{b-a};\;x\in(a,b)
  \end{cases}.\]
\end{definition}
\begin{example}
  A commuter train arrives at a station every half an hour. Let $X$ be the amount of time in minutes that we need to wait for the train to arrive.
  \begin{enumerate}
    \item $X\sim U(0,30).$
      \[f_X(x)=
        \begin{cases}
          0;\;x\notin(0,30)\\ \frac{1}{30};\;x\in(0,30)
      \end{cases}\]
    \item $\PP(5\leq X\leq15)=$ probability that waiting time is in between 5 and 15 $=\int_5^{15}f_X(x)\,dx=\int_5^{15}\frac{1}{30}\,dx=\frac{1}{30}\cdot10=\frac{1}{3}.$
    \item $\PP(X\leq10)=\frac{10}{30}=\frac{1}{3}.$
    \item $\PP(X>10)=1-\PP(X\leq10)=1-\frac{1}{3}=\frac{2}{3}.$
  \end{enumerate}
\end{example}
\subsubsection{Exponential Random Variables}
\begin{definition}
  A continuous random variable $X$ is called an \textbf{exponential random variable} if it has PDF
  \[f_X(x)=
    \begin{cases}
      \lambda e^{-\lambda x};\;x>0\\0;\;o/w
  \end{cases}\]
  where $\lambda>0.$ We write $X\sim Exp(\lambda).$ The CDF of $X$ is given by
  \[F_X(x)=\int_{-\infty}^x f_X(y)\,dy=
    \begin{cases}
      1-e^{-\lambda x};\;x\geq 0\\0;\;x<0
  \end{cases}.\]
\end{definition}
\begin{example}
  The time until the first patient arrives at the emergency room has an exponential distribution with $\lambda =6.9$. Determine the probability that beginning at 6 PM on any given day, the first patient arrives
  \begin{enumerate}
    \item between 6:15 PM and 6:30 PM.
    \item before 7 PM.
  \end{enumerate}
  Solution: Let random variable $X$ denote the time in minutes until the first patient arrives. $X\sim Exp(6.9).$
  \[f_X(x)=
    \begin{cases}
      6.9e^{-6.9x};\;x>0\\0;\;else.
  \end{cases}\]
  \begin{enumerate}
    \item $\PP(\frac{1}{4}\leq X\leq\frac{1}{2})=\int_{\frac{1}{4}}^{\frac{1}{2}}f_X(x)\,dx=\int_{\frac{1}{4}}^{\frac{1}{2}}6.9e^{-6.9x}\,dx=0.146.$ There is a 14.6\% chance that the event happens in that time period.
    \item $\PP(X<1)=\int_{-\infty}^1 f_X(x)\,dx = 0.99.$
  \end{enumerate}
\end{example}
Note: Exponential random variables are the continuous analogue of discrete geometric random variables.
\subsubsection{Lack of Memory Property}
$\PP(X>t+s\mid X>s)=\PP(X>t);s,t\geq0.$ For example, given $s=3,t=2,$ $\PP(X>5\mid X>3)=\PP(X>2).$ The process doesn't remember the first 3 minutes and it starts to count time from beginning. Indeed, $\PP(X>s+t\mid X>s)=\frac{\PP(X>s+t,X>s)}{\PP(X>s)}=\frac{\PP(X>s+t)}{\PP(X>s)}=\frac{e^{-\lambda(t+s)}}{e^{-\lambda s}}=e^{-\lambda t} =\PP(X>t).$\setcounter{example}{3}

\begin{example}
  (continued)
  \begin{enumerate}
      \setcounter{enumi}{2}
    \item Given that the patient doesn't arrive by 6:15 PM, determine the probability that they arrive by 6:45 PM.
  \end{enumerate}
  Solution: $X\sim Exp(6.9).$ We want $\PP(X\leq\frac{3}{4}\mid X>\frac{1}{4}). $ So, $\PP(X\leq \frac{3}{4}\mid X>\frac{1}{4})=1-\PP(X>\frac{3}{4}\mid X>\frac{1}{4})=1-\PP(X>\frac{1}{4}+\frac{2}{4}\mid X>\frac{1}{4})=1-\PP(X>\frac{2}{4})=1-e^{-6.9\frac{1}{2}}=0.968.$

\end{example}
\subsection{Normal(Gaussian) Random Variable}
De-Moivre: bell-shaped curve.
\begin{definition}
  A continuous random variable $X$ is called a normal random variable (Gaussian) if it has a PDF given as
  \[f_X(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}};\;-\infty<x<\infty\]
  where $\mu$ and $\sigma>0$ are real numbers. We say $X$ is normally distributed with parameters $\mu$ and $\sigma^2$.
  \[X\sim N(\mu,\sigma^2).\]
\end{definition}
PDF of normal random variable with parameters $\mu$ and $\sigma^2$ is centered at $\mu$ and its spread depends of $\sigma$. This means $\mu$ is called the location parameter, and $\sigma$ is called the scale parameter. $\E[X]=\mu, var(X)=\sigma^2.$ \\
By FPF, we have $A\subset\R$:
$\PP(X\in A)=\int_A \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x\mu)^2}{2\sigma^2}}\,dx.$\\
So, if $X\sim N(\mu,\sigma^2)$, then $Z=\frac{X-\mu}{\sigma}\sim N(0,1).$\\
Indeed, given $Z\in\R$,
\[F_Z(z)=\PP(Z\leq z)=\PP(\frac{X-\mu}{\sigma}\leq z)=\PP(X\leq\mu+\sigma z)=\int_{-\infty}^{(\mu+\sigma z)}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\,dx.\]

If $\frac{x-\mu}{\sigma}=t,$
\[F_Z(z)=\int_{-\infty}^z \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}\,dt.\]
Hence,
\[f_Z(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}};\;-\infty<z<\infty\implies Z\sim N(0,1).\]
\begin{definition}
  A normal random variable with parameters $0$ and $1$ is called a standard normal random variable, and its PDF and CDF are given by
  \[PDF:\;\phi(Z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^2}{2}};\;-\infty<z<\infty.\]
  \[CDF:\;\Phi(Z)=\int_{-\infty}^z \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}\,dt.\]
\end{definition}
So, if $X\sim N(\mu,\sigma^2)$,
\[\PP(a<X<b)=\PP(\frac{a-\mu}{\sigma}<\frac{X-\mu}{\sigma}<\frac{b-\mu}{\sigma})=\PP(\frac{a-\mu}{\sigma}<Z<\frac{b-\mu}{\sigma})\]\[=\Phi(\frac{b-\mu}{\sigma})-\Phi(\frac{a-\mu}{\sigma}).\]
And,
\[\Phi(-z)=1-\Phi(z).\]
\begin{example}
Let $X\sim N(2,4)$ and $\Phi(2)=0.9772, \Phi(\frac{1}{2})=0.6915.$ a) Find $\PP(1\leq X\leq6).$\\
Solution:
\[\PP(1\leq X\leq6)=\PP(\frac{1-2}{2}\leq Z\leq \frac{6-2}{2})=\PP(-\frac{1}{2}\leq Z\leq 2)=\PP(Z\leq2)-\PP(Z\leq-\frac{1}{2})\]
\[=\Phi(2)-\Phi(\frac{1}{2})=\Phi(2)-(1-\Phi(\frac{1}{2}))=0.6687.\]
b) Find $X_p$ such that $\PP(X\geq X_p)=0.5.$
\[\PP(Z\geq \frac{X_p-2}{2})=0.5\implies1-\PP(Z\leq\frac{X_p-2}{2})=0.5\implies\PP(Z\leq\frac{X_p-2}{2})=0.5\]
\[\implies\frac{X_p-2}{2}=0\implies X_p=2.\]
\end{example}
\subsection{Other Important Continuous Random Variables}
\subsubsection{Gamma Random Variable}
The gamma random variable is the continuous variable version of a Negative Binomial random variable.
\begin{definition}
A continuous random variable $X$ is called a \textbf{gamma random variable} if it has a PDF of the form
\[f_X(x)=
\begin{cases}
  0;\;x\leq0\\ \frac{\lambda^\alpha x^{\alpha-1}e^{-\lambda x}}{\Gamma(\alpha)};\;x>0
\end{cases}\]
$X\sim \Gamma(\alpha,\lambda)$, and its CDF is given by
\[F_X(x)=1-e^{-\lambda x}\sum_{j=0}^{\alpha-1} \frac{(\lambda x)^j}{j!};\;x\geq0.\]
$\Gamma(x)$ is the gamma function, where
\[\Gamma(\alpha)=\int_0^\infty x^{\alpha-1}e^{-x}\,dx;a>0.\]
\end{definition}
\begin{example}
The time until the third patient arrives has the gamma distribution with parameters $\alpha=3, \lambda=6.9$. Determine the probability that, beginning at 6 PM, the third patient arrives between 6:15 and 6:30 PM.\\
Solution: $X\sim\Gamma(3, 6.9)$. We are asked to find $\PP(\frac{1}{4}<X<\frac{1}{2})=
\int_\frac{1}{4}^\frac{1}{2} f_X(x)\,dx$. If we use CDF, $\PP(\frac{1}{4}<X<\frac{1}{2})=F_X(\frac{1}{2})-F_X(\frac{1}{4}).$
\[F_X(x)=1-e^{-6.9x}\sum_{j=1}^2 \frac{(6.9x)^j}{j}=0.420.\]
\end{example}
Special cases:
\begin{enumerate}
\item If $x=1$, $\Gamma(1,\lambda)=Exp(\lambda)$.
\item If $\alpha=\frac{v}{2}$ and $\lambda=\frac{1}{2},$ then $\Gamma(\frac{v}{2},\frac{1}{2})=X^2(v).$
\end{enumerate}
\subsection{Functions of continuous random variables}
$g:$ real-valued function. $X:$ discrete random variable. $Y=g(X)$ will be discrete.\\
But, if $X:$ continuous random variable, then $Y=g(X)$ could be either discrete or continuous. There are a few ways to find the distribution of $Y$.
\subsubsection{Method 1  the CDF Method}
If $g$ is monotone, then
\[F_Y(y)=\PP(Y\leq y)=\PP(g(X)\leq y)=\PP(X\leq g^{-1}(y))=F_X(g^{-1}(y)).\]
\begin{example}
Let $X\sim N(\mu,\sigma^2)$ and $Y=4X-2$. So, $g(X)=Y=4X-2.$ Hence, $f_Y(y)=f_X(\frac{y+2}{4})=\PP(X\leq\frac{y+2}{4})=\int_{-\infty}^\frac{y+2}{4}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\,dx.$
\end{example}
\begin{example}
Let $X\sim N(0,1)$ and $Y=X^2$. We note here that $g(x)=x^2$ and the range of random variable $Y$ is $[0,\infty)$ (Note: $g$ is NOT monotone).
\[F_Y(y)=\PP(Y\leq y)=\PP(X^2\leq y)=\PP(-\sqrt{y}\leq X\leq \sqrt{y})=F_X(\sqrt{y})-F_X(-\sqrt{y}).\]
So, for $y>0$, the PDF of $Y$ is
\[f_Y(y)=F_Y'(y)=F_X'(\sqrt{y})\frac{1}{2\sqrt{y}}-F_X'(-\sqrt{y})(\frac{-1}{2\sqrt{y}})=\frac{1}{2\sqrt{y}}f_X(\sqrt{y})+\frac{1}{2\sqrt{y}}f_X(-\sqrt{y})\]
\[=\frac{1}{\sqrt{y}}f_X(\sqrt{y})=\frac{1}{\sqrt{y}}\frac{1}{\sqrt{2\pi}}e^{-\frac{y}{2}}.\]
So, $X\sim N(0,1),f_X(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},x\in\R.$ $Y=X^2;\;f_Y(y)=\frac{1}{\sqrt{y}}\frac{1}{\sqrt{2\pi}}e^{\frac{-y}{2}}.$ Evidently, even if $Y=X^2,f_Y\neq(f_X)^2$.\\
Here,
\[f_Y(y)=\frac{(\frac{1}{2})^\frac{1}{2}}{\sqrt{\pi}}y^{\frac{1}{2}-1}e^{-\frac{y}{2}}=\frac{(\frac{1}{2})^\frac{1}{2}}{\Gamma(\frac{1}{2})}y^{\frac{1}{2}-1}e^{-\frac{y}{2}}.\;\;(\sqrt{\pi}=\Gamma(\frac{1}{2}))\]
So, $Y\sim \Gamma(\frac{1}{2},\frac{1}{2}): $ $Y$ has the chi-square distribution with 1 degree of freedom. $X\sim N(0,1); Y\sim\chi^2(1).$
\end{example}
\subsubsection{Method 2  the PDF Method}
Let $X$ be a continuous random variable with CDF $F_X$, and $g$ be a real-valued, strictly monotone and differentiable on the range of $X$. Then, the PDF of $Y=g(X)$ is \[f_Y(y)=\frac{f_X(x)}{|g'(x)|}, \text{where }x=g^{-1}(y).\]
\begin{example}
Let $X\sim N(\mu,\sigma^2)$ and $Y=a-3X$. Thus, $g(x)=y=a-3x\implies g'(x)=-3.$\\
Using PDF formula,
\[f_Y(y)=\frac{f_X(x)}{|g'(x)|}=\frac{1}{3}f_X(x)=\frac{1}{3}f_X(\frac{a-y}{3}).\]
Since $X\sim N(\mu,\sigma^2),$
\[f_Y(y)=\frac{1}{3}\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}[\frac{\frac{a-y}{3}-\mu}{\sigma}]^2}=\frac{1}{\sqrt{2\pi(3\sigma^2)}}e^{-\frac{1}{2}[\frac{y-(a-3\mu)}{3\sigma}]^2}.\]
\end{example}
\begin{example}
If $X\sim U(-\frac{\pi}{2},\frac{\pi}{2})$, find a PDF of $Y=\tan(X)$.
\[f_X(x)=
\begin{cases}
  \frac{1}{\pi};\;-\frac{\pi}{2}<x<\frac{\pi}{2}\\0;\;o/w
\end{cases}\]
Here, range of $Y$ is $(-\infty,\infty).$ By the PDF method,
\[f_Y(y)=\frac{1}{|g'(x)|}f_X(x)=\frac{1}{\sec^2x}\cdot\frac{1}{\pi}.\]
So,
\[f_Y(y)=\frac{1}{\sec^2(\tan^{-1}(y))}\cdot\frac{1}{\pi}=\frac{1}{\pi}\cdot\frac{1}{(1+y^2)}.\]
\end{example}
\begin{definition}
A continuous random variable $X$ is called a \textbf{Cauchy random variable} if its PDF is given by
\[f_X(x)=\frac{1}{\pi(1+x^2)};\;-\infty<x<\infty.\]
Note: this distribution will have some unusual properties.
\end{definition}
\section{Jointly Continuous Random Variables}
\begin{definition}
Let $X,Y$ be two random variables. Then, the joint CDF of $X,Y$ is
\[F_{X,Y}(x,y)=\PP(X\leq x,Y\leq y);\;x,y\in\R\]
\[F_{X,Y}:\R^2\rightarrow[0,1].\]
\end{definition}
\subsection{Joint CDFs}
If $f_X(x)=\PP(X\leq x),$ then
\[F_X(x)=\lim_{y\to\infty} F_{X,Y}(x,y)=F_{X,Y}(x,\infty),x\in\R.\]
\[F_Y(y)=\lim_{x\to\infty} F_{X,Y}(x,y)=F_{X,Y}(\infty,y),x\in\R.\]
\subsection{Joint PDFs}
\begin{definition}
Let $X,Y$ be random variables. A non-negative function $f_{X,Y}$ is a \textbf{joint PDF}  if
\[\PP(a\leq X\leq b,c\leq Y\leq d)=\int_a^b \int_c^d f_{X,Y}(x,y)\,dy \,dx.\]
So intuitively,
\[\PP(x\leq X\leq x+\Delta x,y\leq Y\leq y+\Delta y)\approx f_{X,Y}(x,y)\Delta x\Delta y.\]
\end{definition}
Joint PDF is partial derivatives of joint CDF:\\
\textbf{Proposition:} Let $X,Y$ be continuous random variables with join CDF $F_{X,Y}$ and joint PDF $f_{X,Y}$. Then,
\[f_{X,Y}(x,y)=\frac{\partial^2}{\partial x\partial y}F_{X,Y}(x,y)=\frac{\partial^2}{\partial y\partial x}F_{X,Y}(x,y)\]
at the continuity points of partials (if they exist).\\
\textbf{Proposition:} Let $X,Y$ be defined on some sample space with joint PDF $f_{X,Y}(x,y)$. Then,
\[F_{X,Y}(x,y)=\int_{-\infty}^x \int_{-\infty}^y f_{X,Y}(s,t)\,ds \,dt;(x,y)\in\R^2\]
and
\[f_{X,Y}(x,y)=\frac{\partial^2}{\partial x\partial y}F_{X,Y}(x,y),\]
for all points where the mixed derivative exists.
\subsection{Properties of Joint Density Functions}
For joint PDF $f_{X,Y}(x,y)$:
\begin{enumerate}
\item $f_{X,Y}(x,y)\geq 0\;\forall(x,y)\in\R^2.$
\item $\int_{-\infty}^\infty \int_{-\infty}^\infty f_{X,Y}(x,y)\,dx\,dy =1.$
\end{enumerate}
\subsubsection{Fundamental Probability Formula (FPF)}
Suppose that $X,Y$ are two random variables with a joint PDF. Then, for any $A\subset\R^2,$
\[\PP((x,y)\in A)=\iint_A f_{X,Y}(x,y)\,dx\,dy.\]
If $(x,y)\in(-\infty,x]\cdot(-\infty,y],$
\[F_{X,Y}=\PP\{-\infty<X\leq x,-\infty<Y\leq y\}=\int_{-\infty}^x \int_{-\infty}^y f_{X,Y}(s,t)\,dt\,ds.\]
\subsection{Marginal and Conditional PDFs}
\textbf{Proposition:} Let $X,Y$ be two continuous random variables with joint PDF. Then,
\[f_X(x)=\int_{-\infty}^\infty f_{X,Y}(x,y)\,dy.\]
\[f_Y(y)=\int_{-\infty}^\infty f_{X,Y}(x,y)\,dx.\]
By differentiating it,
\[f_X(x)=F_X'(x)=\int_{-\infty}^\infty f_{X,Y}(x,y)\,dy.\]
\begin{example}
Let $X,Y$ denote lifetimes of electrical components A and B. The joint PDF of $X,Y$ is
\[f_{X,Y}(x,y)=\lambda\mu e^{-(\lambda x+\mu y)};\;x,y>0.\]
\begin{enumerate}
\item Determine the probability that both components are functioning at time $t$.
\item Determine the probability that A is the first component to fail.
\item Determine the probability that B is the first component to fail.
\end{enumerate}
\textbf{Solution:}
\begin{enumerate}
\item  Both components functioning at time $t$ means that $X>t,Y>t$. $\PP(X>t,Y>t)=\int_t^\infty \int_t^\infty f_{X,Y}(x,y)\,dx\,dy=\int_t^\infty \int_t^\infty \lambda\mu e^{-(\lambda x+\mu y)}\,dx\,dy=e^{-\mu t}e^{-\lambda t}.$
\item The event that component A is the first to fail, i.e. $\{X<Y\}.$ $\PP(X<Y)=\iint_{x<y} f_{X,Y}(x,y)\,dx\,dy=\int_{x=0}^\infty(\int_{y=x}^\infty \lambda\mu e^{-\lambda x+\mu y)}\,dy)\,dx=\frac{\lambda}{\lambda + \mu}.$
\item Similarly, $\PP(Y<X)=\frac{\mu}{\mu+\lambda}.$
\end{enumerate}
\end{example}
\subsubsection{Conditional PDFs}
Let $X,Y$ be two continuous random variables with joint PDF $f_{X,Y}$.
\[f_{Y\mid X}(y\mid x)=\frac{f_{X,Y}(x,y)}{f_X(x)};\;f_X(x)>0,y\in\R.\]
If $f_X(x)=0$, then $f_{Y\mid X}(y\mid x)=0.$
\subsubsection{Conditional CDFs}
\[F_{Y\mid X}(y\mid x)=\PP(Y\leq y\mid X=x)=\int_{-\infty}^y f_{Y\mid X}(s\mid x)\,ds.\]
So, $\PP(Y\in A\mid X=x)=\int_A f_{Y\mid X}(y\mid x)\,dy.$
\subsection{Independent Continuous Random Variables}
$X,Y$ are independent if for any two subsets $A,B\subset \R,$
\[\PP(X\in A,Y\in B)=\PP(X\in A)\cdot\PP(Y\in B).\]
For discrete random variables, if they are independent, $p_{X,Y}(x,y)=p_X(x)\cdot p_Y(y).$ Thus, for continuous random variables,
\[f_{X,Y}=f_X(x)\cdot f_Y(y).\]
Equivalently, if $f_X(x)>0$,
\[f_{Y\mid X} (y\mid x) = f_Y(y);\; \forall \; y\in \R.\]
\begin{example}
Let $(X,Y)\sim$ Uniform in unit disk.
\[f_{X,Y}(x,y)=
\begin{cases}
\frac{1}{\pi};\;x^2+y^2=1\\0;\;o/w
\end{cases}.\]
The marginal PDF of $X$:
\[f_X(x)=\int_{-\infty}^\infty f_{X,Y}(x,y)\,dy=
\begin{cases}
0;\;x\notin[-1,1]\\\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}\frac{1}{\pi}\,dy ;\;x\in(-1,1)
\end{cases}\]\[=
\begin{cases}
0;\;x\notin[-1,1]\\\frac{2}{\pi}\sqrt{1-x^2} ;\;x\in(-1,1)
\end{cases}.\]
Similarly, the marginal PDF for y is
\[f_Y(y)=
\begin{cases}
0;\;y\notin[-1,1]\\\frac{2}{\pi}\sqrt{1-y^2} ;\;y\in(-1,1)
\end{cases}.\]
The conditional PMF on $X=x$ is
\[f_{Y\mid X=x}(y\mid x)=\frac{f_{X,Y}(x,y)}{f_X(x)}=
\begin{cases}
\frac{1}{\pi}\frac{1}{\frac{2}{\pi}\sqrt{1-x^2}};\;|y|\leq\sqrt{1-x^2}\\0;\;o/w
\end{cases}\]
\[=
\begin{cases}
\frac{1}{2\sqrt{1-x^2}};\;|y|\leq\sqrt{1-x^2}\\0;\;o/w
\end{cases}.\]
So, $Y\mid X=x\sim Unif(-\sqrt{1-x^2},\sqrt{1-x^2}).$ As an example:
\[\PP(X^2+Y^2\leq\frac{1}{4})=\iint_{x^2+y^2\leq\frac{1}{4}}\frac{1}{\pi}\,dx\,dy=\frac{1}{\pi}\cdot\frac{\pi}{4}=\frac{1}{4}.\]
\end{example}
\subsection{Functions of Two or More Continuous Random Variables}
\begin{example}
Let $X_1, X_2\sim Exp(\lambda)$, independently and identically distributed. What is $\PP(X_1+X_2\leq t)?$ (Intuitively, $X_1+X_2\sim \Gamma(2,\lambda).)$\\
Solution: Due to independence, if $Y=X_1+X_2,$
\[f_Y(y)=f_{X_1,X_2}(x_1,x_2)=
\begin{cases}
\lambda^2e^{-\lambda x_1}e^{-\lambda x_2};\;x_1,x_2>0\\0;\;o/w
\end{cases}\implies\]
\[F_Y(y)=\PP(X_1+X_2\leq t)=\iint_A f_{X_1,X_2}(x_1,x_2)\,dx_1\,dx_2\]
\[=\int_{x_2=0}^t\int_{x_1=0}^{t-x_2}\lambda^2e^{-\lambda x_1}e^{-\lambda x_2}\,dx_1\,dx_2=\lambda^2\int_0^te^{-\lambda x_2}(\int_0^{t-x_2}e^{-\lambda x_1}\,dx_1)\,dx_2\]
\[=1-e^{-\lambda t}-\lambda te^{-\lambda t}.\]
So, the PDF of $Y=X_1+X_2$ is
\[f_Y(y)=\frac{d}{dy}F_Y(y)=
\begin{cases}
0;\;y<0\\\lambda^2 ye^{-\lambda y};\;y\geq 0.
\end{cases}\]
\end{example}
\begin{example}
$X_i\sim Exp(\lambda_i),i=1,2,\dots,m.$ $X_i$ can be considered as the lifetimes of $m$ components. Thus, the lifetime of the system is $X=min\{X_1,\dots,X_m\}.$
\end{example}
\textbf{Proposition:} Let $X,Y$ be continuous random variables with a joint PDF. Then a PDF of random variables $X+Y$ can be obtained from either of these two:
\[f_{X+Y}(z)=\int_{-\infty}^\infty f_{X,Y}(x,z-x)\,dx;\;z\in\R.\]
\[f_{X+Y}(z)=\int_{-\infty}^\infty f_{X,Y}(z-y,y)\,dy;\;z\in\R.\]
If $X,Y$ are independent,
\[f_{X+Y}(z)=\int_{-\infty}^\infty f_{X,Y}(x,z-x)\,dx=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\,dx=f_X\cdot f_Y.\]
\begin{example}
Sum of two independent Gamma random variables: Let $X\sim\Gamma(a,\lambda);\;Y\sim\Gamma(b,\lambda);iid.$ Then, $Z=X+Y.$\\
Recall that $\Gamma(\alpha)=\int_0^\infty x^{\alpha-1}e^{-x}\,dx;\alpha>0.$
\\Solution: If $X\sim\Gamma(a,\lambda),$
\[f_X(x)=\frac{\lambda^a}{\Gamma(a)}x^{\alpha-1}e^{-\lambda x};\;x>0.\]
Since $X,Y$ are independent,
\[f_{X+Y}(z)=\int_{-\infty}^\infty f_X(x)f_Y(z-x)\,dx=f_X\cdot f_Y\]
\[=\int_{x=0}^z \frac{\lambda^a}{\Gamma(a)}e^{-\lambda x}x^{a-1}\frac{\lambda^b}{\Gamma(b)}(z-x)^{b-1}e^{-\lambda(z-x)}\,dx\]
\[=\frac{\lambda^{a+b}}{\Gamma(a)\Gamma(b)}e^{-\lambda z}\int_0^z x^{a-1}(z-x)^{b-1}\,dx...\]
We use the substitution $u=\frac{x}{z}$:
\[=\frac{\lambda^{a+b}}{\Gamma(a)\Gamma(b)}e^{-\lambda z}\int_0^1 (uz)^{a-1}(z-uz)^{b-1}z\,du\]
\[=z^{a+b-1}\frac{\lambda^{a+b}}{\Gamma(a)\Gamma(b)}e^{-\lambda z}\int_0^1 u^{a-1}(1-u)^{b-1}\,du...\]
Now, using the fact that $B(a,b)=\int_0^1 u^{a-1}(1-u)^{b-1}\,du=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)},$
\[=\frac{\lambda^{a+b}}{\Gamma(a+b)}e^{-\lambda z}z^{a+b-1}.\]
This is the PDF of $\Gamma(a+b,\lambda). \;\therefore X+Y\sim\Gamma(a+b,\lambda).$
\end{example}
\textbf{Proposition:}
\begin{enumerate}
\item If $X_i\sim\Gamma(a_i,\lambda)$ is independent for $i=1,\dots,m:$
\[\sum_{i=1}^mX_i\sim\Gamma(\sum_{i=1}^ma_i,\lambda).\]
\item If $X_i\sim N(\mu_i,\sigma_i^2)$ is independent for $i=1,\dots,m:$
\[\sum_{i=1}^m X_i\sim N(\sum_{i=1}^m \mu_i,\sum_{i=1}^m \sigma_i^2).\]
\end{enumerate}
\subsection{Multivariate Transformation Theorem}
Recall that in 1 dimension, if $Y=g(X)$, then $f_Y(y)=\frac{f_X(x)}{|g'(x)|};\;x=g^{-1}(y).$\\
In multiple dimensions, $U=g(X,Y),$ and $V=h(X,Y).$ This helps define \[f_{U,V}=\frac{f_{X,Y}(x,y)}{|J(x,y)|}.\]
\textbf{Proposition:} Let $(X,Y)\sim f_{X,Y}(x,y)$ and $U=g(X,Y)$ and $V=h(X,Y),$ where $g,h$ are two real-valued functions, such that
\[J(x,y)=
\begin{vmatrix}
\frac{\partial g}{\partial x}(x,y) && \frac{\partial g}{\partial y}(x,y)\\
\frac{\partial h}{\partial x}(x,y) && \frac{\partial h}{\partial y}(x,y)
\end{vmatrix}\neq0.\]
Then,
\[f_{U,V}(u,v)=\frac{f_{X,Y}(x,y)}{|J(x,y)|}\]
where $(x,y)$ is the unique point in the range of $(x,y)$ such that $g(x,y)=u$ and $h(x,y)=v.$
\begin{example}
Let $X,Y\sim Exp(\lambda);\;iid.$ Let $S=\frac{X}{X+Y}$ and $T=X+Y$. Find $f_{S,T}(s,t).$
\\Solution: By independence,
\[f_{X,Y}(x,y)=\lambda^2e^{-\lambda x}e^{-\lambda y};\;x,y>0.\]
Here, $S=g(X,Y)=\frac{X}{X+Y}$ and $T=h(X,Y)=X+Y.$
\[J(x,y)=
\begin{vmatrix}
\frac{\partial g}{\partial x}(x,y) && \frac{\partial g}{\partial y}(x,y)\\
\frac{\partial h}{\partial x}(x,y) && \frac{\partial h}{\partial y}(x,y)
\end{vmatrix}=
\begin{vmatrix}
\frac{y}{(x+y)^2} && \frac{-x}{(x+y)^2} \\ 1 && 1
\end{vmatrix}=\frac{1}{x+y}=\frac{1}{t}.\]
So, rearranging the equations gives $S=\frac{X}{T}\implies X=ST.$ Therefore, $T=TS+Y\implies T-TS=Y\implies T(1-S)=Y$. Using these, we can find the answer by
\[f_{S,T}(s,t)=\frac{f_{X,Y}(x,y)}{|J(x,y)|}=t\lambda^2e^{-\lambda x}e^{-\lambda y}=t\lambda^2 e^{-\lambda(st)}e^{-\lambda(t(1-s))}\]\[=t\lambda^2e^{-\lambda t};\;t>0,0<s<1.\]
\end{example}
\begin{example}
Let $(X,Y)\sim f_{X,Y}(x,y).$ Find the PDF of $Z=X+Y$.
\\Solution: Let $Z=X+Y$ and $U=X$.
\\Should get
\[f_Z(z)=\int_{-\infty}^\infty f_{Z,U}(z,u)\,du=\int_{-\infty}^\infty f_{X,Y}(u,z-u)\,du.\]
\end{example}
\section{Expected Value of Continuous Random Variables}
\subsection{Expected Value of a Continuous Random Variable}
Recall that in the discrete case, $\E[X]=\sum_x x\cdot\PP(X=x).$ Now, let $X$ be a continuous random variable with PDF $f_X(x).$ The expectation of $X$ is defined by
\[\E[X]=\int_{-\infty}^\infty x\cdot f_X(x)\,dx.\]
\begin{example} Following are some examples:
\begin{enumerate}
\item $X\sim U(a,b).$
\[\E[X]=\int_{-\infty}^\infty xf_X(x)\,dx=\int_a^b x\cdot\frac{1}{b-a}\,dx=\frac{a+b}{2}.\]
\item $X\sim Exp(\lambda).$
\[\E[X]=\int_0^\infty x\cdot\lambda e^{-x}\,dx = \frac{1}{\lambda}\]
\item $X\sim N(\mu,\sigma^2)$, then $\E[X]=\mu.$
\[\E[X]=\int_{-\infty}^\infty x\cdot\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}\,dx=\frac{1}{\sqrt{2\pi\sigma^2}}\int_{-\infty}^\infty x\cdot e^{-\frac{(x-\mu)^2}{2\sigma^2}}\,dx.\]
Let $\frac{x-\mu}{\sigma}=y\implies\,dx=\sigma\,dy$
\[\frac{1}{\sqrt{2\pi}}\int_{-\infty}^\infty (\mu+\sigma y)e^{-\frac{y^2}{2}}\,dy=\mu\int_{-\infty}^\infty e^{-\frac{y^2}{2}}\,dy+\frac{\sigma}{\sqrt{2\pi}}\int_{-\infty}^\infty ye^{-\frac{y^2}{2}}\,dy\]
\[=\frac{\mu}{\sqrt{2\pi}}\int_{-\infty}^\infty e^{-\frac{y^2}{2}}\,dy=\mu.\]
\end{enumerate}
\end{example}
\subsection{Basic Properties of Expected Value}
\begin{enumerate}
\item $\E[c]=c.$
\item $\E[aX+bY]=a\E[X]+b\E[Y].$
\item If $X,Y$ are independent, then $\E[XY]=\E[X]\cdot\E[Y].$
\item Tail probabilities: If $X\geq$ 0 and is continuous, then $\E[X]=\int_0^\infty \PP(X>y)\,dy.$
\end{enumerate}
\begin{example}
Let $X\sim Exp(\lambda).$ Then, $\PP(X>y)=e^{-\lambda y}\implies \E[X]=\frac{1}{\lambda}.$
\end{example}
\subsection{Variance and Covariance}
$var(X)=\E[(X-\E[X])^2]=\E[X^2]-(\E[X])^2=\int_{-\infty}^\infty x^2f_X(x)\,dx-(\int_{-\infty}^\infty xf_X(x)\,dx)^2.$
\\$cov(X,Y)=\E[(X-\E[X])(Y-\E[Y])].$
\begin{example}
Let $X\sim N(0,1).$ Find $var(X)$.
\\Solution: $var(X)=\E[X^2]-(\E[X])^2=\E[X^2].$
\[\E[X^2]=\int_{-\infty}^\infty x^2\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}\,dx=\frac{2}{\sqrt{2\pi}}\int_0^\infty x^2e^{-\frac{x^2}{2}}\,dx=\frac{2}{\sqrt{2\pi}}\int_0^\infty x(xe^{-\frac{x^2}{2}})\,dx.\]
\[=1.\]
\end{example}
\subsection{Conditional Expectation}
$\E[Y\mid X=x]=\int_{-\infty}^\infty yf_{Y\mid X}(y\mid x)\,dx.$
\subsection{Generating functions and Limit Theorems}
\begin{definition}
The moment generating function (MGF) of $X$ is defined as
\[M_X(t)=\E[e^{tx}],\,t\in\R.\]
If $X$ is a continuous random variable,
\[M_X(t)=\int_{-\infty}^\infty e^{tx}f_X(x)\,dx.\]
\end{definition}
Note: $M_X(t)$ is defined for all $t\in\R$ for which the random variable $e^{tx}$ has finite expectation.
\begin{example}
Given $X\sim Bin(n,p)$, find its MGF.
\[p_X(x)=\binom{n}{x}p^x(1-p)^{n-x},\,x=0,1,\dots,n.\]
\[M_X(t)=\sum_x e^{tx}p_X(x)=\sum_{x=0}^n e^{tx}\binom{n}{x}p^x(1-p)^{n-x}=(pe^t+1-p)^n.\]
\end{example}
\begin{example}
Let $X\sim Exp(\lambda).$ Find the domain of MGF of $X$.
\[M_X(t)=\int_0^\infty e^{tx}f_X(x)\,dx=\lambda\int_0^\infty e^{tx}e^{-\lambda x}\,dx=\frac{\lambda}{\lambda-t};\;\lambda>t.\]
The domain of MGF for $Exp(\lambda)$ is $\{t\in\R\mid t<\lambda\}.$
\end{example}
\subsection{Generating Moments}
The $r^{th}$ moment is given by $\E[X^r];\;r\in\N.$\\
Here, let $X$ be a random variable. Then, $M_X(t)=\E[e^{tX}].$ Now,
\[\frac{d}{dt}M_X(t)=\frac{d}{dt}\E[e^{tX}]=\E[\frac{\partial}{\partial t}e^{tX}]=\E[Xe^{tX}]\implies\]
\[\frac{d^r}{dt^r}M_X(t)=\E[X^re^{tX}].\]
At $t=0,$
\[M_X'(0)=\E[X^r].\]
So,
\[\E[X]=M_X'(0).\]
\[\E[X^2]=M_X''(0).\]
\begin{example}
Let $X\sim N(\mu,\sigma^2).$
\[M_X(t)=e^{\mu t+\frac{\sigma^2 t^2}{2}}\implies\E[X]=M_X'(0)=\mu\text{ and }\E[X^2]=M_X''(0)=\sigma^2.\]
\end{example}
\section{Limit Theorems}
\begin{definition}
Let $X_1,\dots,X_n$ be $iid$ random variables with common finite mean $\mu.$ Then, the \textbf{Law of Large Numbers} tells us that $\forall\,\epsilon>0,$
\[\lim_{n\to{\infty}}\PP(|\frac{X_1+\dots+X_n}{n}-\mu|<\epsilon)=1.\]
\end{definition}
\begin{definition}
Let $X_1,X_2,\dots$ be $iid$ random variables from an \textit{unknown} distribution with finite mean and finite variance. Let $S_n=X_1+\dots+X_n$ and $Y_n=\frac{S_n-\E[Sn]}{\sqrt{var(S_n)}}=\frac{S_n-n\mu}{\sqrt{n\sigma^2}}.$
\\What happens to the distribution of $Y_n$ as $n\to\infty?$
\[\lim_{n\to\infty}\PP(Y_n\leq t)=\PP(Z\leq t);\;Z\sim N(0,1).\]
The \textbf{Central Limit Theorem} tells us that as $n\to\infty$, the distribution of $Y_n$ becomes Normal.
\[\lim_{n\to\infty}\PP(\frac{\bar X_n -\mu}{\frac{\sigma}{\sqrt{n}}}\leq t)=\PP(Z\leq t).\]
\end{definition}
\end{document}